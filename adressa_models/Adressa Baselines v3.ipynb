{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4248661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe97bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import scipy\n",
    "#from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input,Flatten, Embedding, Reshape, Multiply, Dropout, Dense, Concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Layer, SpatialDropout1D, GlobalMaxPooling1D, Bidirectional, GRU\n",
    "from tensorflow.keras.layers import Dot, TimeDistributed, BatchNormalization, multiply\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#import keras.backend as K\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import math\n",
    "import pickle\n",
    "import collections\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ef9d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/adressa_v2/\"\n",
    "with open(PATH + \"articles_v3.bin\", \"rb\") as f_in:\n",
    "    articles = pickle.load(f_in)\n",
    "# two different files: behaviors.bin and behaviors_two_days.bin\n",
    "with open(PATH + \"full_behaviors.bin\", \"rb\") as f_in:\n",
    "    behaviors = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07fde94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640503\n"
     ]
    }
   ],
   "source": [
    "print(len(behaviors[\"userId\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f19892",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame(np.random.randn(len(behaviors), 2))\n",
    "msk = np.random.rand(len(df_)) < 0.2\n",
    "print(len(behaviors))\n",
    "behaviors = behaviors[msk]\n",
    "print(len(behaviors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c4597",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2aaa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors[\"time\"] = pd.to_datetime(behaviors[\"time\"], unit=\"s\")\n",
    "behaviors = behaviors.drop_duplicates([\"userId\", \"id\"])\n",
    "print(\"before merge: \",len(behaviors))\n",
    "behaviors = behaviors.drop(columns=[\"title\"])\n",
    "articles.rename(columns={\"article_id\": \"id\"}, inplace=True)\n",
    "behaviors = behaviors.merge(articles, on=[\"id\"])\n",
    "print(\"after merge:\",len(behaviors))\n",
    "\n",
    "print(\"Len before removal: \",len(behaviors))\n",
    "behaviors = behaviors[behaviors.groupby('userId').userId.transform('count')>2].copy()\n",
    "print(\"Len after removal: \",len(behaviors))\n",
    "\n",
    "\n",
    "user_enc = LabelEncoder()\n",
    "article_enc = LabelEncoder()\n",
    "behaviors[\"user_id\"] = user_enc.fit_transform(behaviors[\"userId\"].values)\n",
    "behaviors[\"article_id\"] = article_enc.fit_transform(behaviors[\"id\"].values)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d86c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# Helper functions\n",
    "def _removeNonAscii(s):\n",
    "    return \"\".join(i for i in s if  ord(i)<128)\n",
    "\n",
    "def make_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"norwegian\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def text_to_list(text):\n",
    "    text = text.split(\" \")\n",
    "    return text\n",
    "\n",
    "def take_one_category(text):\n",
    "    temp = text.split()\n",
    "    if len(temp) > 1:\n",
    "        return temp[1]\n",
    "    return temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title(df):\n",
    "    df[\"title_cleaned\"] = df.title.apply(func = make_lower_case)\n",
    "    df[\"title_cleaned\"] = df.title_cleaned.apply(func = remove_stop_words)\n",
    "    df[\"title_cleaned\"] = df.title_cleaned.apply(func = remove_punctuation)\n",
    "    return df\n",
    "def hyphen_to_underline(category):\n",
    "    \"\"\"\n",
    "    Convert hyphen to underline for the subcategories. So that Tfidf works correctly\n",
    "    \"\"\"\n",
    "    return category.replace(\"-\",\"_\")\n",
    "#behaviors = clean_title(behaviors)\n",
    "behaviors[\"category_cleaned\"] = behaviors[\"kw_category\"].apply(func = take_one_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4f514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_enc = LabelEncoder()\n",
    "subcategory_enc = LabelEncoder()\n",
    "behaviors[\"category_int\"] = subcategory_enc.fit_transform(behaviors[\"category_cleaned\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba50f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = behaviors[\"user_id\"].unique()\n",
    "userid_to_profile = collections.defaultdict(list)\n",
    "for user_id in tqdm(users):\n",
    "    user_subcat = behaviors[behaviors[\"user_id\"] == user_id][\"category_int\"].values.tolist()\n",
    "    counter = Counter(user_subcat)\n",
    "    s = sorted(user_subcat, key=lambda x: (counter[x], x), reverse=True)\n",
    "    final_subcategories = []\n",
    "    for elem in s:\n",
    "        if elem not in final_subcategories:\n",
    "            final_subcategories.append(elem)\n",
    "    while len(final_subcategories) < 6:\n",
    "        final_subcategories.append(0)\n",
    "    userid_to_profile[user_id] = final_subcategories[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca3cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_df = pd.DataFrame.from_dict(userid_to_profile, orient=\"index\")\n",
    "profile_df[\"user_id\"] = profile_df.index\n",
    "behaviors = behaviors.merge(profile_df, on=\"user_id\")\n",
    "behaviors = behaviors.rename(columns={\"0\": \"p0\",\"1\": \"p1\",\"2\": \"p2\",\"3\": \"p3\",\"4\": \"p4\",\"5\": \"p5\",})\n",
    "\n",
    "article_id_to_category_int = behaviors[[\"article_id\", \"category_int\"]].set_index(\"article_id\").to_dict()\n",
    "article_id_to_category_int = article_id_to_category_int[\"category_int\"]\n",
    "\n",
    "behaviors.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429d4265",
   "metadata": {},
   "source": [
    "# 2. Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5537cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "SAMPLE_SIZE = 99\n",
    "NUM_NEGATIVES = 4\n",
    "ALL_ARTICLE_IDS = behaviors[\"article_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8259d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors[\"article_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = behaviors[[\"user_id\", \"article_id\"]]\n",
    "rating = [1 for i in range(len(interactions))]\n",
    "interactions = interactions.assign(label=pd.Series(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891bd179",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(interactions)) <0.8\n",
    "train = interactions[msk]\n",
    "test = interactions[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7479ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = interactions.set_index(\"user_id\")\n",
    "train = train.set_index(\"user_id\")\n",
    "test = test.set_index(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(train_df, user_id, article_id):\n",
    "    \"\"\"\n",
    "    Negative sample training instance; for each positive instance, add 4 negative articles\n",
    "    \n",
    "    Return user_ids, news_ids, category_1, category_2, authors_onehotencoded, titles\n",
    "    \"\"\"\n",
    "    \n",
    "    users, articles, labels = [], [], []\n",
    "    user_item_set = set(zip(train_df.index.values, train_df[article_id].values))\n",
    "    for (u,i) in user_item_set:\n",
    "        for _ in range(NUM_NEGATIVES):\n",
    "            negative_item = np.random.choice(ALL_ARTICLE_IDS)\n",
    "            while (u, negative_item) in user_item_set:\n",
    "                negative_item = np.random.choice(ALL_ARTICLE_IDS)\n",
    "            users.append(u)\n",
    "            articles.append(negative_item)\n",
    "            labels.append(0)\n",
    "        users.append(u)\n",
    "        articles.append(i)\n",
    "        labels.append(1)\n",
    "    \n",
    "    users, articles, labels = shuffle(users, articles, labels, random_state=0)\n",
    "    return users[:40000], articles[:40000], labels[:40000]\n",
    "\n",
    "train_users, train_articles, train_labels = negative_sampling(train, \"user_id\", \"article_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eadfc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(list(zip(train_users, train_articles, train_labels)), columns=[\"user_id\", \"article_ids\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04fad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_items_interacted(user_id, interactions_df=behaviors):\n",
    "    interacted_items = interactions_df.loc[user_id][\"article_id\"]\n",
    "    return set(interacted_items if type(interacted_items) == pd.Series else [interacted_items])\n",
    "\n",
    "def get_not_interacted(user_id, interactions_df=behaviors):\n",
    "    interacted_items = get_items_interacted(user_id, interactions_df)\n",
    "    all_items = set(behaviors[\"article_id\"])\n",
    "    not_interacted_items = all_items - interacted_items\n",
    "    random.seed(SEED)\n",
    "    not_interacted_items = random.sample(not_interacted_items, SAMPLE_SIZE)\n",
    "    return not_interacted_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df1a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(behaviors[\"user_id\"].unique())\n",
    "num_items = len(behaviors[\"article_id\"].unique())\n",
    "dims = 20\n",
    "def get_model_neumf(num_users, num_items, dims, dense_layers=[128, 64, 32, 8]):\n",
    "    user_input = Input(shape=(1,), name=\"user\")\n",
    "    item_input = Input(shape=(1,), name=\"item\")\n",
    "    \n",
    "    mf_user_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=num_users, \n",
    "                            input_length=1, \n",
    "                            embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_user_emb\")(user_input)\n",
    "    mf_item_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=num_items, \n",
    "                            input_length=1, \n",
    "                            embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_item_emb\")(item_input)\n",
    "    \n",
    "    num_layers = len(dense_layers)\n",
    "    mlp_user_emb = Embedding(output_dim=int(dense_layers[0] / 2), \n",
    "                             input_dim=num_users, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                             embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"mlp_user_emb\")(user_input)\n",
    "    mlp_item_emb = Embedding(output_dim=int(dense_layers[0] / 2), \n",
    "                             input_dim=num_items, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                             embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"mlp_user_item\")(item_input)\n",
    "    \n",
    "    # Matrix factorization\n",
    "    mf_user_vecs = Reshape([dims])(mf_user_emb)\n",
    "    mf_item_vecs = Reshape([dims])(mf_item_emb)\n",
    "    \n",
    "    mf_vec = multiply([mf_user_vecs, mf_item_vecs])\n",
    "    \n",
    "    #MLP\n",
    "    mlp_vec = Concatenate()([mlp_user_emb, mlp_item_emb])\n",
    "    mlp_vector = Flatten()(mlp_vec)\n",
    "    \n",
    "    for num_nodes in dense_layers:\n",
    "        l = Dense(num_nodes, activation=\"relu\")\n",
    "        mlp_vector = l(mlp_vector)\n",
    "    \n",
    "    y = Concatenate()([mf_vec, mlp_vector])\n",
    "    y = Dense(1, activation=\"sigmoid\", name=\"pred\")(y)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input], outputs=y)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.01),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_neumf = get_model_neumf(num_users, num_items, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d94269",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_input, articles_input, labels_input = np.array(train_users).reshape(-1,1), np.array(train_articles).reshape(-1,1), np.array(train_labels).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_ids = train.index.unique().values\n",
    "\n",
    "#user_input = df_train.iloc[:, 0].values.reshape((-1,1))\n",
    "#profile_input = df_train.iloc[:, 1:6].values\n",
    "#item_input = df_train.iloc[:, 7].values.reshape((-1,1))\n",
    "#labels = df_train.iloc[:, 8].values.reshape((-1,1))\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "hits_list = []\n",
    "ndcg_list = []\n",
    "best_hits = 0\n",
    "best_ndcgs = 0\n",
    "best_hits_five = 0\n",
    "best_ndcgs_five = 0\n",
    "\n",
    "epochs=4\n",
    "for epoch in range(epochs):\n",
    "    hist = model_neumf.fit([users_input, articles_input], labels_input, epochs=1, shuffle=True, verbose=1, batch_size=32)\n",
    "    \n",
    "    train_loss.append(hist.history[\"loss\"])\n",
    "    train_acc.append(hist.history[\"accuracy\"])\n",
    "    #val_loss.append(hist.history[\"val_loss\"])\n",
    "    #val_acc.append(hist.history[\"val_accuracy\"])\n",
    "    \n",
    "    #hits, ndcgs, hits_five, ndcgs_five = evalaute_model_neumf( model_neumf, df_test, userid_to_true_item)\n",
    "    #hits_list.append(np.average(hits))\n",
    "    #ndcg_list.append(np.average(ndcgs))\n",
    "    \n",
    "    #temp_hits = np.average(hits)\n",
    "    #temp_ndcgs = np.average(ndcgs)\n",
    "    #if (temp_hits > best_hits):\n",
    "    #    best_hits = temp_hits\n",
    "    #    best_ndcgs = temp_ndcgs\n",
    "    #    best_hits_five = np.average(hits_five)\n",
    "    #    best_ndcgs_five = np.average(ndcgs_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27aacc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = test.index.values[:1000]\n",
    "test_items = test.article_id.values[:1000]\n",
    "test_set = zip(test_users, test_items)\n",
    "hits = []\n",
    "for (u,i) in tqdm(test_set):\n",
    "    not_interacted_items = get_not_interacted(u)\n",
    "    users = np.array([u]*100).astype(int)\n",
    "    items = np.array([i] + not_interacted_items)\n",
    "    np.random.shuffle(items)\n",
    "    #items = random.sample(items, len(items))\n",
    "    predictions = model_neumf.predict([users, items])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    print(i)\n",
    "    print(items)\n",
    "    top_ten_items = [items[k] for k in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    if i in top_ten_items:\n",
    "        hits.append(1)\n",
    "    else:\n",
    "        hits.append(0)\n",
    "print(np.average(hits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e68daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = test.index.values[0]\n",
    "i = test.article_id.values[0]\n",
    "\n",
    "not_interacted_items = get_not_interacted(u)\n",
    "users = np.array([u]*100)\n",
    "items = np.array([i] + not_interacted_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe27b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31baa299",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_neumf.predict([users, items])\n",
    "predicted_labels = np.squeeze(predictions)\n",
    "top_ten_items = [items[k] for k in np.argsort(predicted_labels)[::-1][0:10].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d590e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14162b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = test.index.values\n",
    "test_items = test.article_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f21d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822945f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
