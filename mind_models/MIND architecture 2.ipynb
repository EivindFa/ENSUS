{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b492587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import scipy\n",
    "#from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input,Flatten, Embedding, Reshape, Multiply, Dropout, Dense, Concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Layer, SpatialDropout1D, GlobalMaxPooling1D, Bidirectional, GRU\n",
    "from tensorflow.keras.layers import Dot, TimeDistributed, BatchNormalization\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#import keras.backend as K\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd2c809",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/mind_small/\"\n",
    "news = pd.read_csv(PATH + \"news.tsv\",header=None, sep=\"\\t\")\n",
    "behaviors = pd.read_csv(PATH + \"behaviors.tsv\", header=None, sep=\"\\t\")\n",
    "news.columns = [\"news_id\", \"category\", \"sub_category\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"]\n",
    "behaviors.columns = [\"idx\", \"user_id\", \"time\", \"history\", \"impressions\"]\n",
    "behaviors = behaviors.drop_duplicates([\"user_id\", \"history\"]) \n",
    "behaviors.dropna(subset=[\"user_id\", \"history\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27e8d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = {}\n",
    "for idx, row in behaviors.iterrows():\n",
    "    sessions[row[\"user_id\"]] = row[\"history\"].split(\" \")\n",
    "\n",
    "users = []\n",
    "clicks = []\n",
    "for k, v in sessions.items():\n",
    "    for elem in v:\n",
    "        users.append(k)\n",
    "        clicks.append(elem)\n",
    "\n",
    "tuples = list(zip(users, clicks))\n",
    "interactions = pd.DataFrame(tuples, columns=[\"user\", \"news_id\"])\n",
    "interactions = interactions[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b0e7f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>news_id</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U13740</td>\n",
       "      <td>N55189</td>\n",
       "      <td>tv</td>\n",
       "      <td>tvnews</td>\n",
       "      <td>'Wheel Of Fortune' Guest Delivers Hilarious, O...</td>\n",
       "      <td>We'd like to solve the puzzle, Pat: Blair Davi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAIORni.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U10045</td>\n",
       "      <td>N55189</td>\n",
       "      <td>tv</td>\n",
       "      <td>tvnews</td>\n",
       "      <td>'Wheel Of Fortune' Guest Delivers Hilarious, O...</td>\n",
       "      <td>We'd like to solve the puzzle, Pat: Blair Davi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAIORni.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U85394</td>\n",
       "      <td>N55189</td>\n",
       "      <td>tv</td>\n",
       "      <td>tvnews</td>\n",
       "      <td>'Wheel Of Fortune' Guest Delivers Hilarious, O...</td>\n",
       "      <td>We'd like to solve the puzzle, Pat: Blair Davi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAIORni.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U78244</td>\n",
       "      <td>N55189</td>\n",
       "      <td>tv</td>\n",
       "      <td>tvnews</td>\n",
       "      <td>'Wheel Of Fortune' Guest Delivers Hilarious, O...</td>\n",
       "      <td>We'd like to solve the puzzle, Pat: Blair Davi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAIORni.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U27024</td>\n",
       "      <td>N55189</td>\n",
       "      <td>tv</td>\n",
       "      <td>tvnews</td>\n",
       "      <td>'Wheel Of Fortune' Guest Delivers Hilarious, O...</td>\n",
       "      <td>We'd like to solve the puzzle, Pat: Blair Davi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAIORni.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user news_id category sub_category  \\\n",
       "0  U13740  N55189       tv       tvnews   \n",
       "1  U10045  N55189       tv       tvnews   \n",
       "2  U85394  N55189       tv       tvnews   \n",
       "3  U78244  N55189       tv       tvnews   \n",
       "4  U27024  N55189       tv       tvnews   \n",
       "\n",
       "                                               title  \\\n",
       "0  'Wheel Of Fortune' Guest Delivers Hilarious, O...   \n",
       "1  'Wheel Of Fortune' Guest Delivers Hilarious, O...   \n",
       "2  'Wheel Of Fortune' Guest Delivers Hilarious, O...   \n",
       "3  'Wheel Of Fortune' Guest Delivers Hilarious, O...   \n",
       "4  'Wheel Of Fortune' Guest Delivers Hilarious, O...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  We'd like to solve the puzzle, Pat: Blair Davi...   \n",
       "1  We'd like to solve the puzzle, Pat: Blair Davi...   \n",
       "2  We'd like to solve the puzzle, Pat: Blair Davi...   \n",
       "3  We'd like to solve the puzzle, Pat: Blair Davi...   \n",
       "4  We'd like to solve the puzzle, Pat: Blair Davi...   \n",
       "\n",
       "                                             url title_entities  \\\n",
       "0  https://assets.msn.com/labs/mind/AAIORni.html             []   \n",
       "1  https://assets.msn.com/labs/mind/AAIORni.html             []   \n",
       "2  https://assets.msn.com/labs/mind/AAIORni.html             []   \n",
       "3  https://assets.msn.com/labs/mind/AAIORni.html             []   \n",
       "4  https://assets.msn.com/labs/mind/AAIORni.html             []   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0  [{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...  \n",
       "1  [{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...  \n",
       "2  [{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...  \n",
       "3  [{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...  \n",
       "4  [{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = interactions.merge(news, on=[\"news_id\"])\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70725d9",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7de393ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len before removal:  10000\n",
      "Len after removal:  9867\n"
     ]
    }
   ],
   "source": [
    "# remove users which have fewer than 5 interacations\n",
    "print(\"Len before removal: \",len(merged))\n",
    "_keys = merged[\"user\"].value_counts()[merged[\"user\"].value_counts() > 5].keys()\n",
    "merged = merged[merged[\"user\"].isin(_keys)]\n",
    "print(\"Len after removal: \",len(merged))\n",
    "\n",
    "\n",
    "user_enc = LabelEncoder()\n",
    "article_enc = LabelEncoder()\n",
    "merged[\"user_id\"] = user_enc.fit_transform(merged[\"user\"].values)\n",
    "merged[\"article_id\"] = article_enc.fit_transform(merged[\"news_id\"].values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f8ec796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# Helper functions\n",
    "def _removeNonAscii(s):\n",
    "    return \"\".join(i for i in s if  ord(i)<128)\n",
    "\n",
    "def make_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def text_to_list(text):\n",
    "    text = text.split(\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd11d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title(df):\n",
    "    df[\"title_cleaned\"] = df.title.apply(func = make_lower_case)\n",
    "    df[\"title_cleaned\"] = df.title_cleaned.apply(func = remove_stop_words)\n",
    "    df[\"title_cleaned\"] = df.title_cleaned.apply(func = remove_punctuation)\n",
    "    return df\n",
    "def hyphen_to_underline(category):\n",
    "    \"\"\"\n",
    "    Convert hyphen to underline for the subcategories. So that Tfidf works correctly\n",
    "    \"\"\"\n",
    "    return category.replace(\"-\",\"_\")\n",
    "merged = clean_title(merged)\n",
    "merged[\"subcategory_cleaned\"] = merged[\"sub_category\"].apply(func = hyphen_to_underline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df2af00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9867x164 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9867 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=\"word\", tokenizer=str.split)\n",
    "item_ids = merged[\"article_id\"].unique().tolist()\n",
    "tfidf_matrix = vectorizer.fit_transform(merged[\"subcategory_cleaned\"])\n",
    "tfidf_feature_names = vectorizer.get_feature_names()\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55c7ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ids = merged[\"article_id\"].tolist()\n",
    "\n",
    "def get_item_profile(item_id):\n",
    "    \"\"\"\n",
    "    item_id: the news article id\n",
    "    Return: an array of each n-gram in the item article. \n",
    "        with their n-gram id in tfidf_feature_names and weight.\n",
    "    \"\"\"\n",
    "    idx = item_ids.index(item_id) # returns the index to the item id\n",
    "    item_profile = tfidf_matrix[idx:idx+1]\n",
    "    return item_profile\n",
    "    \n",
    "def get_item_profiles(ids):\n",
    "    #print(ids)\n",
    "    item_profiles_list = [get_item_profile(x) for x in ids]\n",
    "    item_profiles = scipy.sparse.vstack(item_profiles_list)\n",
    "    return item_profiles\n",
    "\n",
    "def build_user_profile(person_id):\n",
    "    interactions = merged[merged[\"user_id\"] == person_id][\"article_id\"].values # gets all articles\n",
    "    user_item_profiles = get_item_profiles(interactions)\n",
    "    user_item_profiles = np.sum(user_item_profiles, axis=0)\n",
    "    user_profile_norm = sklearn.preprocessing.normalize(user_item_profiles)\n",
    "    return user_item_profiles\n",
    "    \n",
    "#t = build_user_profile(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "830900db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:08<00:00, 28.30it/s]\n"
     ]
    }
   ],
   "source": [
    "def calculate_user_profiles(unique_user_ids):\n",
    "    user_profiles = {}\n",
    "    for idx in tqdm(unique_user_ids):\n",
    "        token_relevance = build_user_profile(idx).tolist()[0]\n",
    "        zipped = zip(tfidf_feature_names, token_relevance)\n",
    "        s = sorted(zipped, key=lambda x: -x[-1])[:6]\n",
    "        user_profiles[idx] = s\n",
    "    return user_profiles\n",
    "        \n",
    "user_profiles = calculate_user_profiles(merged[\"user_id\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51dc4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "subcategory_to_id = {name: idx+1 for idx, name in enumerate(tfidf_feature_names)}\n",
    "id_to_subcategory = {idx: name for name, idx in subcategory_to_id.items()}\n",
    "id_to_subcategory[0] = \"Null\"\n",
    "subcategory_to_id[\"Null\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58f4c9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9867it [00:00, 11501.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# add all id-category to the userprofile in df\n",
    "profile_array = []\n",
    "for index, row in tqdm(merged.iterrows()):\n",
    "    \n",
    "    user_idx = row[\"user_id\"]\n",
    "    profile = user_profiles[user_idx]\n",
    "    temp = []\n",
    "    for keyword_tuple in profile:\n",
    "        temp.append(subcategory_to_id[keyword_tuple[0]])\n",
    "    profile_array.append(temp)\n",
    "merged[\"profile\"] = profile_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "090a4be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the id-category to the news articles\n",
    "merged[\"subcategory_to_int\"] = [subcategory_to_id[cat] for cat in merged[\"subcategory_cleaned\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7df51053",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_unique = merged.drop_duplicates(\"user_id\")\n",
    "userid_to_profile = user_unique[[\"user_id\", \"profile\"]].set_index(\"user_id\").to_dict()[\"profile\"]\n",
    "\n",
    "category_enc = LabelEncoder()\n",
    "merged[\"main_category_int\"] = category_enc.fit_transform(merged[\"category\"].values)\n",
    "article_id_to_category_int = merged[[\"article_id\", \"main_category_int\"]].set_index(\"article_id\").to_dict()\n",
    "article_id_to_category_int = article_id_to_category_int[\"main_category_int\"]\n",
    "\n",
    "article_id_to_subcategory_int = merged[[\"article_id\", \"subcategory_to_int\"]].set_index(\"article_id\").to_dict()\n",
    "article_id_to_subcategory_int = article_id_to_subcategory_int[\"subcategory_to_int\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c98ef2",
   "metadata": {},
   "source": [
    "# 2. Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5f8871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, user_id, article_id, have_timestamp, timestamp):\n",
    "    \"\"\"\n",
    "    params: \n",
    "        col_1: user_id\n",
    "        col_2: article_id\n",
    "    \"\"\"\n",
    "    df_test = df\n",
    "    if have_timestamp: # if df have timestamp; take last interacted article into test set\n",
    "        df_test = df_test.sort_values(timestamp).groupby(user_id).tail(1)\n",
    "    else:\n",
    "        df_test = df_test.sort_values(user_id).groupby(user_id).tail(1)\n",
    "    df_train = df.drop(index=df_test.index)\n",
    "    \n",
    "    assert df_test.shape[0] + df_train.shape[0] == df.shape[0]\n",
    "    \n",
    "    return df_train, df_test\n",
    "df_train_true, df_test_true = train_test_split(merged, \"user_id\", \"article_id\", False, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3b8f5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:00<00:00, 1433.83it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_userid_to_article_history(df):\n",
    "    userid_to_article_history = {}\n",
    "    for user_id in tqdm(df[\"user_id\"].unique()):\n",
    "        click_history = df[df[\"user_id\"] == user_id][\"article_id\"].values\n",
    "        if len(click_history) < 30:\n",
    "            while len(click_history) < 30:\n",
    "                click_history = np.append(click_history, 0)\n",
    "        if len(click_history) > 30:\n",
    "            click_history = click_history[:30]\n",
    "        userid_to_article_history[user_id] = click_history\n",
    "    return userid_to_article_history\n",
    "userid_to_article_history = get_userid_to_article_history(df_train_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a08bae38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9500/9500 [00:00<00:00, 35520.39it/s]\n"
     ]
    }
   ],
   "source": [
    "all_article_ids = merged[\"article_id\"].unique()\n",
    "\n",
    "def negative_sampling(train_df, all_article_ids, user_id, article_id):\n",
    "    \"\"\"\n",
    "    Negative sample training instance; for each positive instance, add 4 negative articles\n",
    "    \n",
    "    Return user_ids, news_ids, category_1, category_2, authors_onehotencoded, titles\n",
    "    \"\"\"\n",
    "    \n",
    "    user_click_history, articles, article_category, labels = [], [], [], []\n",
    "    p0, p1, p2, p3, p4, p5, p6, p7, p8, p9 = [], [], [], [], [], [], [], [], [], []\n",
    "    user_item_set = set(zip(train_df[user_id], \n",
    "                            train_df[article_id]))\n",
    "    num_negatives = 4\n",
    "\n",
    "    for (u, i) in tqdm(user_item_set):\n",
    "        user_click_history.append(userid_to_article_history[u])\n",
    "        profile = np.array(userid_to_profile[u])\n",
    "        p0.append(profile[0])\n",
    "        p1.append(profile[1])\n",
    "        p2.append(profile[2])\n",
    "        p3.append(profile[3])\n",
    "        p4.append(profile[4])\n",
    "        p5.append(profile[5])\n",
    "        \n",
    "        articles.append(i)\n",
    "        labels.append(1)\n",
    "        for _ in range(num_negatives):\n",
    "            negative_item = np.random.choice(all_article_ids)\n",
    "            while (u, negative_item) in user_item_set:\n",
    "                negative_item = np.random.choice(all_article_ids)\n",
    "            user_click_history.append(userid_to_article_history[u])\n",
    "            p0.append(profile[0])\n",
    "            p1.append(profile[1])\n",
    "            p2.append(profile[2])\n",
    "            p3.append(profile[3])\n",
    "            p4.append(profile[4])\n",
    "            p5.append(profile[5])\n",
    "            articles.append(negative_item)\n",
    "            labels.append(0)\n",
    "    \n",
    "    user_click_history, p0, p1, p2, p3, p4, p5, articles, labels = shuffle(user_click_history, p0, p1, p2, p3, p4, p5, articles, labels, random_state=0)\n",
    "\n",
    "    return pd.DataFrame(list(zip(user_click_history,p0, p1, p2, p3, p4, p5, articles, labels)), columns=[\"user_history\",\"p0\", \"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"article_id\", \"labels\"])\n",
    "\n",
    "\n",
    "\n",
    "df_train = negative_sampling(df_train_true, all_article_ids, \"user_id\", \"article_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3189003b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 39.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>article_id</th>\n",
       "      <th>labels</th>\n",
       "      <th>user_history_0</th>\n",
       "      <th>user_history_1</th>\n",
       "      <th>...</th>\n",
       "      <th>user_history_20</th>\n",
       "      <th>user_history_21</th>\n",
       "      <th>user_history_22</th>\n",
       "      <th>user_history_23</th>\n",
       "      <th>user_history_24</th>\n",
       "      <th>user_history_25</th>\n",
       "      <th>user_history_26</th>\n",
       "      <th>user_history_27</th>\n",
       "      <th>user_history_28</th>\n",
       "      <th>user_history_29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119</td>\n",
       "      <td>123</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>45</td>\n",
       "      <td>66</td>\n",
       "      <td>1931</td>\n",
       "      <td>1</td>\n",
       "      <td>1739</td>\n",
       "      <td>234</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>45</td>\n",
       "      <td>123</td>\n",
       "      <td>62</td>\n",
       "      <td>74</td>\n",
       "      <td>124</td>\n",
       "      <td>3342</td>\n",
       "      <td>0</td>\n",
       "      <td>4222</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>1763</td>\n",
       "      <td>3738</td>\n",
       "      <td>241</td>\n",
       "      <td>3380</td>\n",
       "      <td>3684</td>\n",
       "      <td>3930</td>\n",
       "      <td>1292</td>\n",
       "      <td>2301</td>\n",
       "      <td>1471</td>\n",
       "      <td>2510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>147</td>\n",
       "      <td>60</td>\n",
       "      <td>93</td>\n",
       "      <td>39</td>\n",
       "      <td>66</td>\n",
       "      <td>1610</td>\n",
       "      <td>0</td>\n",
       "      <td>3563</td>\n",
       "      <td>1469</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123</td>\n",
       "      <td>89</td>\n",
       "      <td>147</td>\n",
       "      <td>94</td>\n",
       "      <td>113</td>\n",
       "      <td>124</td>\n",
       "      <td>392</td>\n",
       "      <td>0</td>\n",
       "      <td>3563</td>\n",
       "      <td>2786</td>\n",
       "      <td>...</td>\n",
       "      <td>1182</td>\n",
       "      <td>3753</td>\n",
       "      <td>4505</td>\n",
       "      <td>3618</td>\n",
       "      <td>3660</td>\n",
       "      <td>830</td>\n",
       "      <td>3387</td>\n",
       "      <td>515</td>\n",
       "      <td>2175</td>\n",
       "      <td>3380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>113</td>\n",
       "      <td>123</td>\n",
       "      <td>124</td>\n",
       "      <td>74</td>\n",
       "      <td>126</td>\n",
       "      <td>132</td>\n",
       "      <td>1060</td>\n",
       "      <td>0</td>\n",
       "      <td>2606</td>\n",
       "      <td>4140</td>\n",
       "      <td>...</td>\n",
       "      <td>2429</td>\n",
       "      <td>2319</td>\n",
       "      <td>3648</td>\n",
       "      <td>3887</td>\n",
       "      <td>1780</td>\n",
       "      <td>4131</td>\n",
       "      <td>3614</td>\n",
       "      <td>1965</td>\n",
       "      <td>1528</td>\n",
       "      <td>1339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    p0   p1   p2  p3   p4   p5  article_id  labels  user_history_0  \\\n",
       "0  119  123   11  14   45   66        1931       1            1739   \n",
       "1   82   45  123  62   74  124        3342       0            4222   \n",
       "2   62  147   60  93   39   66        1610       0            3563   \n",
       "3  123   89  147  94  113  124         392       0            3563   \n",
       "4  113  123  124  74  126  132        1060       0            2606   \n",
       "\n",
       "   user_history_1  ...  user_history_20  user_history_21  user_history_22  \\\n",
       "0             234  ...                0                0                0   \n",
       "1              28  ...             1763             3738              241   \n",
       "2            1469  ...                0                0                0   \n",
       "3            2786  ...             1182             3753             4505   \n",
       "4            4140  ...             2429             2319             3648   \n",
       "\n",
       "   user_history_23  user_history_24  user_history_25  user_history_26  \\\n",
       "0                0                0                0                0   \n",
       "1             3380             3684             3930             1292   \n",
       "2                0                0                0                0   \n",
       "3             3618             3660              830             3387   \n",
       "4             3887             1780             4131             3614   \n",
       "\n",
       "   user_history_27  user_history_28  user_history_29  \n",
       "0                0                0                0  \n",
       "1             2301             1471             2510  \n",
       "2                0                0                0  \n",
       "3              515             2175             3380  \n",
       "4             1965             1528             1339  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fix_dftrain(df, column, max_len, padding):\n",
    "    i = 0\n",
    "    for i in tqdm(range(max_len)):\n",
    "        df[column + \"_\" + str(i)] = df[column].apply(lambda x: x[i] if i < len(x) else padding)\n",
    "    #df.drop(column, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "df_train = fix_dftrain(df_train, \"user_history\", 30, 0)\n",
    "df_train.drop(columns=[\"user_history\"], inplace=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "739d06be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:00<00:00, 531.23it/s]\n",
      "/Users/eivindfalun/opt/anaconda3/envs/shapRs/lib/python3.7/site-packages/ipykernel_launcher.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "# For each user; for each item the user has interacted with in the test set;\n",
    "    # Sample 99 items the user has not interacted with in the past and add the one test item  \n",
    "    \n",
    "def negative_sample_testset(ordiginal_df, df_test, all_article_ids, user_id, article_id):\n",
    "    test_user_item_set = set(zip(df_test[user_id], df_test[article_id]))\n",
    "    user_interacted_items = ordiginal_df.groupby(user_id)[article_id].apply(list).to_dict()\n",
    "    users = []\n",
    "    p0, p1, p2, p3, p4, p5, p6, p7, p8, p9 = [], [], [], [], [], [], [], [], [], []\n",
    "    res_arr = []\n",
    "    \n",
    "    userid_to_true_item = {} # keep track of the real items\n",
    "    for (u,i) in tqdm(test_user_item_set):\n",
    "        interacted_items = user_interacted_items[u]\n",
    "        not_interacted_items = set(all_article_ids) - set(interacted_items)\n",
    "        selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99))\n",
    "        test_items = selected_not_interacted + [i]\n",
    "        temp = []\n",
    "        profile = userid_to_profile[u]\n",
    "        for j in range(len(test_items)):\n",
    "            temp.append([u,userid_to_article_history[u], profile[0],profile[1],profile[2],profile[3],profile[4],profile[5], test_items[j]])\n",
    "        #            user_click_history.append(userid_to_article_history[u])\n",
    "\n",
    "        res_arr.append(temp)\n",
    "        userid_to_true_item[u] = i \n",
    "    X_test = np.array(res_arr)\n",
    "    X_test = X_test.reshape(-1, X_test.shape[-1])\n",
    "    df_test = pd.DataFrame(X_test, columns=[\"user_id\",\"click_history\", \"p0\", \"p1\", \"p2\", \"p3\", \"p4\", \"p5\",\"article_id\"])\n",
    "    return X_test, df_test, userid_to_true_item\n",
    "X_test, df_test, userid_to_true_item = negative_sample_testset(merged, df_test_true, merged[\"article_id\"].unique(), \"user_id\", \"article_id\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e849766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 78.80it/s]\n"
     ]
    }
   ],
   "source": [
    "def fix_dftest(df, column, max_len, padding):\n",
    "    i = 0\n",
    "    for i in tqdm(range(max_len)):\n",
    "        df[column + \"_\" + str(i)] = df[column].apply(lambda x: x[i] if i < len(x) else padding)\n",
    "    #df.drop(column, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "df_test = fix_dftest(df_test, \"click_history\", 30, 0)\n",
    "df_test.drop(columns=[\"click_history\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac05415c",
   "metadata": {},
   "source": [
    "# 3. Evaluation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c87b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f0d9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_rating(model, user_id, user_profiles, all_articles,user_clicks, true_item):\n",
    "    ### Reshaping to make it on the right shape ###\n",
    "    #expanded_user_id = np.array([user_id]*100).reshape((100,1))\n",
    "    all_articles = np.array(all_articles).reshape(-1,1)\n",
    "    \n",
    "    # predictions\n",
    "    predictions = model.predict([user_clicks, user_profiles, all_articles])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [all_articles[i] for i in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    hr = getHitRatio(top_ten_items, true_item)\n",
    "    ndcg = getNDCG(top_ten_items, true_item)\n",
    "    return hr, ndcg\n",
    "\n",
    "def evalaute_model(model, df_test, userid_to_true_item):\n",
    "    print(\"Evaluate model\")\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    users = df_test[\"user_id\"].unique()\n",
    "    for user_id in tqdm(users):\n",
    "        user_df = df_test[df_test[\"user_id\"] == user_id] # get the 100 samples for this user\n",
    "        true_item = userid_to_true_item[user_id] # get the actual true item in the test set\n",
    "        all_articles = user_df[\"article_id\"].values.astype(\"int64\") # get all possible articles\n",
    "        user_profiles = user_df.iloc[:, 1:7].values.astype(\"int64\")# get the user_profile\n",
    "        user_clicks = user_df.iloc[:, 8:].values.astype(\"int64\")\n",
    "        \n",
    "        ht, ndcg = evaluate_one_rating(model, user_id, user_profiles, all_articles,user_clicks, true_item)\n",
    "        hits.append(ht)\n",
    "        ndcgs.append(ndcg)\n",
    "    return hits, ndcgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734cfb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a1ea5ba",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29b9a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "num_unique_categories = len(subcategory_to_id)\n",
    "num_users = len(merged[\"user_id\"].unique()) +1\n",
    "num_items = len(merged[\"article_id\"].unique()) + 1\n",
    "dims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e15de5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.autograph.experimental.do_not_convert\n",
    "def get_model(num_users, num_items, dims, dense_layers=[128, 64, 32, 8]):\n",
    "    user_history = Input(shape=(30,), name=\"user\")\n",
    "    user_profile_input = Input(shape=(6,), name=\"profile\")\n",
    "    item_input = Input(shape=(1,), name=\"item\")\n",
    "    \n",
    "    mf_user_emb = Embedding(output_dim=dims, input_dim=num_items+1, input_length=30, name=\"mf_user_emb\")(user_history)\n",
    "    mf_profile_emb = Embedding(output_dim=dims, input_dim=num_unique_categories, input_length=6, name=\"mf_profile_emb\")(user_profile_input)\n",
    "    mf_item_emb = Embedding(output_dim=dims, input_dim=num_items+1, input_length=1, name=\"mf_item_emb\")(item_input)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #profile_emb = GlobalAveragePooling1D()(mf_profile_emb)\n",
    "    profile_vecs = Flatten()(mf_user_emb)\n",
    "    user_vecs = Flatten()(mf_profile_emb)\n",
    "    item_vecs = Reshape([dims])(mf_item_emb)\n",
    "    \n",
    "    user_vecs_complete = Concatenate(axis=1)([user_vecs, profile_vecs])\n",
    "    input_vecs = Concatenate()([user_vecs_complete, item_vecs])\n",
    "    x = Dense(128, activation=\"relu\", name=\"dense_0\")(input_vecs)\n",
    "    x = Dropout(0.5)(x)\n",
    "    y = Dense(1, activation=\"sigmoid\", name=\"prediction\")(x)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_history, user_profile_input, item_input], outputs=y)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.01),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = get_model(num_users, num_items, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e40c0ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47500, 30) (47500, 6) (47500, 1) (47500, 1)\n"
     ]
    }
   ],
   "source": [
    "###### Training ########\n",
    "user_history = df_train.iloc[:, 8:].values.astype(\"int64\")\n",
    "profile_input = df_train.iloc[:, 0:6].values.astype(\"int64\")\n",
    "item_input = df_train.iloc[:, 6].values.reshape((-1,1)).astype(\"int64\")\n",
    "labels = df_train.iloc[:, 7].values.reshape((-1,1)).astype(\"int64\")\n",
    "print(user_history.shape,profile_input.shape, item_input.shape, labels.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "587c6c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336/1336 [==============================] - 3s 2ms/step - loss: 0.5266 - accuracy: 0.7946 - val_loss: 0.4950 - val_accuracy: 0.7998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/231 [00:00<00:24,  9.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:09<00:00, 23.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  26/1336 [..............................] - ETA: 2s - loss: 0.4806 - accuracy: 0.8065"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336/1336 [==============================] - 3s 2ms/step - loss: 0.4902 - accuracy: 0.8050 - val_loss: 0.4937 - val_accuracy: 0.8078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/231 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:09<00:00, 24.02it/s]\n"
     ]
    }
   ],
   "source": [
    "all_user_ids = merged[\"user_id\"].unique()\n",
    "\n",
    "epochs=2\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "hits_list = []\n",
    "ndcg_list = []\n",
    "best_hits = 0\n",
    "best_ndcgs = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    hist = model.fit([user_history, profile_input, item_input], labels, epochs=1, shuffle=True, verbose=1, validation_split=0.1)\n",
    "    \n",
    "    train_loss.append(hist.history[\"loss\"])\n",
    "    train_acc.append(hist.history[\"accuracy\"])\n",
    "    val_loss.append(hist.history[\"val_loss\"])\n",
    "    val_acc.append(hist.history[\"val_accuracy\"])\n",
    "    \n",
    "    hits, ndcgs = evalaute_model( model, df_test, userid_to_true_item)\n",
    "    hits_list.append(np.average(hits))\n",
    "    ndcg_list.append(np.average(ndcgs))\n",
    "    \n",
    "    temp_hits = np.average(hits)\n",
    "    temp_ndcgs = np.average(ndcgs)\n",
    "    if (temp_hits > best_hits):\n",
    "        best_hits = temp_hits\n",
    "    if temp_ndcgs > best_ndcgs:\n",
    "        best_ndcgs = temp_ndcgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fceac4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit @ 10: 0.40\n",
      "ncdgs @ 10: 0.24\n"
     ]
    }
   ],
   "source": [
    "print(\"Hit @ 10: {:.2f}\".format(best_hits))\n",
    "print(\"ncdgs @ 10: {:.2f}\".format(best_ndcgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5356266",
   "metadata": {},
   "source": [
    "# 5. user profile eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45435f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(user_id, df):\n",
    "    #user_history, profile_input, item_input\n",
    "    ## Setup ###\n",
    "    \n",
    "    click_history = userid_to_article_history[user_id]\n",
    "    user_profile = get_user_profile(df, user_id)\n",
    "    display_items = df[df[\"user_id\"] == user_id][\"article_id\"].values.reshape(-1, 1).astype(\"int64\")\n",
    "    user_profile = np.tile(user_profile, display_items.shape[0]).reshape(-1, 6).astype(\"int64\")\n",
    "    click_history = np.tile(np.array(click_history), display_items.shape[0]).reshape(-1,30).astype(\"int64\")\n",
    "    \n",
    "    ## Preds ###\n",
    "    predictions = model.predict([click_history, user_profile,display_items])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [display_items[i][0] for i in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    return top_ten_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4076b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_subcategory(article_id, df):\n",
    "    \"\"\"\n",
    "    Return the article's category\n",
    "        type: int\n",
    "    \"\"\"\n",
    "    return article_id_to_subcategory_int[article_id]\n",
    "def get_category_hit_ratio(user_profile, top_ten_categories):\n",
    "    for profile in user_profile:\n",
    "        for category in top_ten_categories:\n",
    "            if profile == category:\n",
    "                return 1\n",
    "    return 0\n",
    "def get_ndcgs_category(user_profile, top_ten_categories):\n",
    "    for i in range(len(top_ten_categories)):\n",
    "        item = top_ten_categories[i]\n",
    "        for profile in user_profile:\n",
    "            if item == profile:\n",
    "                return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "def get_user_profile(df, user_id):\n",
    "    \"\"\"\n",
    "    Return the user profile given user_id\n",
    "    \"\"\"\n",
    "    return df[df[\"user_id\"] == user_id].iloc[0, 1:7].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c58262c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:08<00:00, 25.78it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_category_hits_ndcg(df):\n",
    "    hits_ten,ndcgs_ten = [], []\n",
    "    hits_five, ndcgs_five = [], []\n",
    "    \n",
    "    for user_id in tqdm(df[\"user_id\"].unique()):\n",
    "        top_ten_articles = get_recommendations(user_id, df)\n",
    "        top_ten_subcategories = [get_article_subcategory(_id, df) for _id in top_ten_articles]\n",
    "        user_profile = userid_to_profile[user_id]\n",
    "\n",
    "        hit_ten = get_category_hit_ratio(user_profile, top_ten_subcategories)\n",
    "        ndcg_ten = get_ndcgs_category(user_profile, top_ten_subcategories)\n",
    "        \n",
    "        hit_five = get_category_hit_ratio(user_profile, top_ten_subcategories[:5])\n",
    "        ndcg_five = get_ndcgs_category(user_profile, top_ten_subcategories[:5])\n",
    "        \n",
    "        hits_ten.append(hit_ten)\n",
    "        ndcgs_ten.append(ndcg_ten)\n",
    "        \n",
    "        hits_five.append(hit_five)\n",
    "        ndcgs_five.append(ndcg_five)\n",
    "        \n",
    "        \n",
    "    return np.average(hits_ten), np.average(ndcgs_ten), np.average(hits_five), np.average(ndcgs_five)\n",
    "        \n",
    "        \n",
    "category_hits_ten, category_ndcg_ten, category_hits_five, category_ndcg_five = get_category_hits_ndcg(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54dfaab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9134199134199135\n",
      "0.5966653263543462\n",
      "0.7878787878787878\n",
      "0.5555950706702306\n"
     ]
    }
   ],
   "source": [
    "print(category_hits_ten)\n",
    "print(category_ndcg_ten)\n",
    "print(category_hits_five)\n",
    "print(category_ndcg_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6c9203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
