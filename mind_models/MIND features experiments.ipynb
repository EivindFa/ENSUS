{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8574f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import scipy\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Input,Flatten, Embedding, Reshape, Multiply, Dropout, Dense, Concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Layer, SpatialDropout1D, GlobalMaxPooling1D, Bidirectional, GRU, LSTM\n",
    "from tensorflow.keras.layers import Dot, TimeDistributed, BatchNormalization, Add, multiply\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#import keras.backend as K\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import math\n",
    "import collections\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d741285",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a568ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/mind_small/\"\n",
    "news = pd.read_csv(PATH + \"news.tsv\",header=None, sep=\"\\t\")\n",
    "behaviors = pd.read_csv(PATH + \"behaviors.tsv\", header=None, sep=\"\\t\")\n",
    "news.columns = [\"news_id\", \"category\", \"sub_category\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"]\n",
    "behaviors.columns = [\"idx\", \"user_id\", \"time\", \"history\", \"impressions\"]\n",
    "behaviors = behaviors.drop_duplicates([\"user_id\", \"history\"]) \n",
    "behaviors.dropna(subset=[\"user_id\", \"history\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695461b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = {}\n",
    "for idx, row in behaviors.iterrows():\n",
    "    sessions[row[\"user_id\"]] = row[\"history\"].split(\" \")\n",
    "\n",
    "users = []\n",
    "clicks = []\n",
    "for k, v in sessions.items():\n",
    "    for elem in v:\n",
    "        users.append(k)\n",
    "        clicks.append(elem)\n",
    "\n",
    "tuples = list(zip(users, clicks))\n",
    "interactions = pd.DataFrame(tuples, columns=[\"user\", \"news_id\"])\n",
    "interactions = interactions[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a989e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = interactions.merge(news, on=[\"news_id\"])\n",
    "merged.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d67b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged))\n",
    "merged = merged.drop_duplicates()\n",
    "print(len(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb70970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.dropna(subset=[\"abstract\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce756ff",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb2945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove users which have fewer than 5 interacations\n",
    "print(\"Len before removal: \",len(merged))\n",
    "_keys = merged[\"user\"].value_counts()[merged[\"user\"].value_counts() > 5].keys()\n",
    "merged = merged[merged[\"user\"].isin(_keys)]\n",
    "print(\"Len after removal: \",len(merged))\n",
    "\n",
    "\n",
    "user_enc = LabelEncoder()\n",
    "article_enc = LabelEncoder()\n",
    "merged[\"user_id\"] = user_enc.fit_transform(merged[\"user\"].values)\n",
    "merged[\"article_id\"] = article_enc.fit_transform(merged[\"news_id\"].values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# Helper functions\n",
    "def _removeNonAscii(s):\n",
    "    return \"\".join(i for i in s if  ord(i)<128)\n",
    "\n",
    "def make_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def text_to_list(text):\n",
    "    text = text.split(\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd79f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title(df):\n",
    "    df[\"title_cleaned\"] = df.title.apply(func = make_lower_case)\n",
    "    df[\"title_cleaned\"] = df.title_cleaned.apply(func = remove_stop_words)\n",
    "    df[\"title_cleaned\"] = df.title_cleaned.apply(func = remove_punctuation)\n",
    "    return df\n",
    "def clean_abstract(df):\n",
    "    df[\"abstract_cleaned\"] = df.abstract.apply(func = make_lower_case)\n",
    "    df[\"abstract_cleaned\"] = df.abstract_cleaned.apply(func = remove_stop_words)\n",
    "    df[\"abstract_cleaned\"] = df.abstract_cleaned.apply(func = remove_punctuation)\n",
    "    return df\n",
    "def hyphen_to_underline(category):\n",
    "    \"\"\"\n",
    "    Convert hyphen to underline for the subcategories. So that Tfidf works correctly\n",
    "    \"\"\"\n",
    "    return category.replace(\"-\",\"_\")\n",
    "merged = clean_title(merged)\n",
    "merged = clean_abstract(merged)\n",
    "merged[\"subcategory_cleaned\"] = merged[\"sub_category\"].apply(func = hyphen_to_underline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a25a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d73b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_enc = LabelEncoder()\n",
    "subcategory_enc = LabelEncoder()\n",
    "merged[\"subcategory_int\"] = subcategory_enc.fit_transform(merged[\"subcategory_cleaned\"].values)\n",
    "merged[\"category_int\"] = subcategory_enc.fit_transform(merged[\"category\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa79375",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = merged[\"user_id\"].unique()\n",
    "userid_to_profile = collections.defaultdict(list)\n",
    "for user_id in tqdm(users):\n",
    "    user_subcat = merged[merged[\"user_id\"] == user_id][\"subcategory_int\"].values.tolist()\n",
    "    counter = Counter(user_subcat)\n",
    "    s = sorted(user_subcat, key=lambda x: (counter[x], x), reverse=True)\n",
    "    final_subcategories = []\n",
    "    for elem in s:\n",
    "        if elem not in final_subcategories:\n",
    "            final_subcategories.append(elem)\n",
    "    while len(final_subcategories) < 6:\n",
    "        final_subcategories.append(0)\n",
    "    userid_to_profile[user_id] = final_subcategories[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da07cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_df = pd.DataFrame.from_dict(userid_to_profile, orient=\"index\")\n",
    "profile_df[\"user_id\"] = profile_df.index\n",
    "merged = merged.merge(profile_df, on=\"user_id\")\n",
    "merged = merged.rename(columns={\"0\": \"p0\",\"1\": \"p1\",\"2\": \"p2\",\"3\": \"p3\",\"4\": \"p4\",\"5\": \"p5\",})\n",
    "\n",
    "article_id_to_category_int = merged[[\"article_id\", \"category_int\"]].set_index(\"article_id\").to_dict()\n",
    "article_id_to_category_int = article_id_to_category_int[\"category_int\"]\n",
    "\n",
    "article_id_to_subcategory_int = merged[[\"article_id\", \"subcategory_int\"]].set_index(\"article_id\").to_dict()\n",
    "article_id_to_subcategory_int = article_id_to_subcategory_int[\"subcategory_int\"]\n",
    "\n",
    "merged.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a87c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "MAXLEN=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bac7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(merged[\"title_cleaned\"].values)\n",
    "temp = tokenizer.texts_to_sequences(merged[\"title_cleaned\"].values)\n",
    "temp = pad_sequences(temp, padding=\"post\", maxlen=MAXLEN)\n",
    "merged[\"title_tokenized\"] = temp.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words_title = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aeed7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4621b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_abstract = Tokenizer()\n",
    "tokenizer_abstract.fit_on_texts(merged[\"abstract_cleaned\"].values)\n",
    "temp = tokenizer.texts_to_sequences(merged[\"abstract_cleaned\"].values)\n",
    "temp = pad_sequences(temp, padding=\"post\", maxlen=MAXLEN)\n",
    "merged[\"abstract_tokenized\"] = temp.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ffa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words_abstract = len(tokenizer_abstract.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b63139",
   "metadata": {},
   "outputs": [],
   "source": [
    "articleId_to_title = merged[[\"article_id\", \"title_tokenized\"]].set_index(\"article_id\").to_dict()[\"title_tokenized\"]\n",
    "article_to_category = merged[[\"article_id\", \"category_int\"]].set_index(\"article_id\").to_dict()[\"category_int\"]\n",
    "article_to_subcategory = merged[[\"article_id\", \"subcategory_int\"]].set_index(\"article_id\").to_dict()[\"subcategory_int\"]\n",
    "article_to_abstract = merged[[\"article_id\", \"abstract_tokenized\"]].set_index(\"article_id\").to_dict()[\"abstract_tokenized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a48487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items_interacted(user_id, df):\n",
    "    interacted_items = df[df[\"user_id\"]==user_id][\"article_id\"]\n",
    "    return set(interacted_items if type(interacted_items) == pd.Series else [interacted_items])\n",
    "SAMPLE_SIZE=99\n",
    "def get_not_interacted(user_id, df):\n",
    "    interacted_items = get_items_interacted(user_id, df)\n",
    "    all_items = set(df[\"article_id\"])\n",
    "    not_interacted_items = all_items - interacted_items\n",
    "    random.seed(SEED)\n",
    "    not_interacted_items = random.sample(not_interacted_items, SAMPLE_SIZE)\n",
    "    return not_interacted_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad5475c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb318db3",
   "metadata": {},
   "source": [
    "# 2.  Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45add55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, user_id, article_id, have_timestamp, timestamp):\n",
    "    \"\"\"\n",
    "    params: \n",
    "        col_1: user_id\n",
    "        col_2: article_id\n",
    "    \"\"\"\n",
    "    df_test = df\n",
    "    if have_timestamp: # if df have timestamp; take last interacted article into test set\n",
    "        df_test = df_test.sort_values(timestamp).groupby(user_id).tail(1)\n",
    "    else:\n",
    "        df_test = df_test.sort_values(user_id).groupby(user_id).tail(1)\n",
    "    df_train = df.drop(index=df_test.index)\n",
    "    \n",
    "    assert df_test.shape[0] + df_train.shape[0] == df.shape[0]\n",
    "    \n",
    "    return df_train, df_test\n",
    "df_train_true, df_test_true = train_test_split(merged, \"user_id\", \"article_id\", False, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df59de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_userid_to_article_history(df):\n",
    "    userid_to_article_history = {}\n",
    "    for user_id in tqdm(df[\"user_id\"].unique()):\n",
    "        click_history = df[df[\"user_id\"] == user_id][\"article_id\"].values\n",
    "        if len(click_history) < 10:\n",
    "            while len(click_history) < 10:\n",
    "                click_history = np.append(click_history, 0)\n",
    "        if len(click_history) > 10:\n",
    "            click_history = click_history[:10]\n",
    "        userid_to_article_history[user_id] = click_history\n",
    "    return userid_to_article_history\n",
    "userid_to_article_history = get_userid_to_article_history(df_train_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2950deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_article_ids = merged[\"article_id\"].unique()\n",
    "\n",
    "def negative_sampling(train_df, all_article_ids, user_id, article_id):\n",
    "    \"\"\"\n",
    "    Negative sample training instance; for each positive instance, add 4 negative articles\n",
    "    \n",
    "    Return user_ids, news_ids, category_1, category_2, authors_onehotencoded, titles\n",
    "    \"\"\"\n",
    "    \n",
    "    user_ids, user_click_history, articles, article_category, article_sub_category,titles,abstract, labels = [],[],[], [], [], [], [], []\n",
    "    p0, p1, p2, p3, p4, p5, p6, p7, p8, p9 = [], [], [], [], [], [], [], [], [], []\n",
    "    user_item_set = set(zip(train_df[user_id], \n",
    "                            train_df[article_id]))\n",
    "    num_negatives = 4\n",
    "\n",
    "    for (u, i) in tqdm(user_item_set):\n",
    "        user_ids.append(u)\n",
    "        user_click_history.append(userid_to_article_history[u])\n",
    "        profile = np.array(userid_to_profile[u])\n",
    "        p0.append(profile[0])\n",
    "        p1.append(profile[1])\n",
    "        p2.append(profile[2])\n",
    "        p3.append(profile[3])\n",
    "        p4.append(profile[4])\n",
    "        p5.append(profile[5])\n",
    "        article_category.append(article_id_to_category_int[i])\n",
    "        article_sub_category.append(article_id_to_subcategory_int[i])\n",
    "        titles.append(articleId_to_title[i])\n",
    "        abstract.append(article_to_abstract[i])\n",
    "        \n",
    "        for _ in range(num_negatives):\n",
    "            negative_item = np.random.choice(all_article_ids)\n",
    "            while (u, negative_item) in user_item_set:\n",
    "                negative_item = np.random.choice(all_article_ids)\n",
    "            user_ids.append(u)\n",
    "            user_click_history.append(userid_to_article_history[u])\n",
    "            p0.append(profile[0])\n",
    "            p1.append(profile[1])\n",
    "            p2.append(profile[2])\n",
    "            p3.append(profile[3])\n",
    "            p4.append(profile[4])\n",
    "            p5.append(profile[5])\n",
    "            \n",
    "            article_category.append(article_id_to_category_int[negative_item])\n",
    "            article_sub_category.append(article_id_to_subcategory_int[negative_item])\n",
    "            titles.append(articleId_to_title[negative_item])\n",
    "            abstract.append(article_to_abstract[negative_item])\n",
    "            \n",
    "            articles.append(negative_item)\n",
    "            labels.append(0)\n",
    "        articles.append(i)\n",
    "        labels.append(1)\n",
    "    \n",
    "    user_ids, user_click_history, p0, p1, p2, p3, p4, p5, articles,article_category,article_sub_category,titles,abstract, labels = shuffle(user_ids,user_click_history, p0, p1, p2, p3, p4, p5, articles,article_category,article_sub_category,titles,abstract, labels, random_state=0)\n",
    "\n",
    "    return pd.DataFrame(list(zip(user_ids,user_click_history,p0, p1, p2, p3, p4, p5, articles,article_category,article_sub_category,titles,abstract, labels)), columns=[\"user_id\",\"user_history\",\"p0\", \"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"article_id\",\"article_category\",\"article_sub_category\",\"titles\",\"abstract\", \"labels\"])\n",
    "\n",
    "\n",
    "\n",
    "df_train = negative_sampling(df_train_true, all_article_ids, \"user_id\", \"article_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae6c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dftrain(df, column, max_len, padding):\n",
    "    i = 0\n",
    "    for i in tqdm(range(max_len)):\n",
    "        df[column + \"_\" + str(i)] = df[column].apply(lambda x: x[i] if i < len(x) else padding)\n",
    "    #df.drop(column, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "df_train = fix_dftrain(df_train, \"user_history\", 10, 0)\n",
    "df_train.drop(columns=[\"user_history\"], inplace=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a62a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each user; for each item the user has interacted with in the test set;\n",
    "    # Sample 99 items the user has not interacted with in the past and add the one test item  \n",
    "    \n",
    "def negative_sample_testset(ordiginal_df, df_test, all_article_ids, user_id, article_id):\n",
    "    test_user_item_set = set(zip(df_test[user_id], df_test[article_id]))\n",
    "    user_interacted_items = ordiginal_df.groupby(user_id)[article_id].apply(list).to_dict()\n",
    "    users = []\n",
    "    p0, p1, p2, p3, p4, p5, p6, p7, p8, p9 = [], [], [], [], [], [], [], [], [], []\n",
    "    res_arr = []\n",
    "    article_category, article_sub_category = [], []\n",
    "    \n",
    "    userid_to_true_item = {} # keep track of the real items\n",
    "    for (u,i) in tqdm(test_user_item_set):\n",
    "        interacted_items = user_interacted_items[u]\n",
    "        not_interacted_items = set(all_article_ids) - set(interacted_items)\n",
    "        selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99))\n",
    "        test_items = selected_not_interacted + [i]\n",
    "        temp = []\n",
    "        profile = userid_to_profile[u]\n",
    "        for j in range(len(test_items)):\n",
    "            temp.append([u,\n",
    "                         userid_to_article_history[u], \n",
    "                         profile[0],\n",
    "                         profile[1],\n",
    "                         profile[2],\n",
    "                         profile[3],\n",
    "                         profile[4],\n",
    "                         profile[5], \n",
    "                         test_items[j], \n",
    "                         article_id_to_category_int[test_items[j]],\n",
    "                         article_id_to_subcategory_int[test_items[j]], \n",
    "                         articleId_to_title[test_items[j]],\n",
    "                         article_to_abstract[test_items[j]]\n",
    "                        ])\n",
    "        #            user_click_history.append(userid_to_article_history[u])\n",
    "\n",
    "        res_arr.append(temp)\n",
    "        userid_to_true_item[u] = i \n",
    "    X_test = np.array(res_arr)\n",
    "    X_test = X_test.reshape(-1, X_test.shape[-1])\n",
    "    df_test = pd.DataFrame(X_test, columns=[\"user_id\",\n",
    "                                            \"click_history\", \n",
    "                                            \"p0\", \n",
    "                                            \"p1\", \n",
    "                                            \"p2\", \n",
    "                                            \"p3\", \n",
    "                                            \"p4\", \n",
    "                                            \"p5\",\n",
    "                                            \"article_id\", \n",
    "                                            \"category\", \n",
    "                                            \"sub_category\",\n",
    "                                            \"title\",\n",
    "                                            \"abstract\"])\n",
    "    return X_test, df_test, userid_to_true_item\n",
    "X_test, df_test, userid_to_true_item = negative_sample_testset(merged, df_test_true, merged[\"article_id\"].unique(), \"user_id\", \"article_id\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de0b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dftest(df, column, max_len, padding):\n",
    "    i = 0\n",
    "    for i in tqdm(range(max_len)):\n",
    "        df[column + \"_\" + str(i)] = df[column].apply(lambda x: x[i] if i < len(x) else padding)\n",
    "    #df.drop(column, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "df_test = fix_dftest(df_test, \"click_history\", 10, 0)\n",
    "df_test.drop(columns=[\"click_history\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936840e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78641316",
   "metadata": {},
   "source": [
    "# 4. NeuMF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b96d6b4",
   "metadata": {},
   "source": [
    "# 4.1 NeuMF no features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb97a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6028e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(merged[\"user_id\"].unique())\n",
    "num_items = len(merged[\"article_id\"].unique())\n",
    "dims = 20\n",
    "def get_model_neumf(num_users, num_items, dims, dense_layers=[128, 64, 32, 8]):\n",
    "    user_input = Input(shape=(1,), name=\"user\")\n",
    "    item_input = Input(shape=(1,), name=\"item\")\n",
    "    \n",
    "    mf_user_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=num_users, \n",
    "                            input_length=1, \n",
    "                            embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_user_emb\")(user_input)\n",
    "    mf_item_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=num_items,\n",
    "                            embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            input_length=1, name=\"mf_item_emb\")(item_input)\n",
    "    \n",
    "    num_layers = len(dense_layers)\n",
    "    mlp_user_emb = Embedding(output_dim=int(dense_layers[0] / 2), \n",
    "                             input_dim=num_users, \n",
    "                             input_length=1,\n",
    "                             embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"mlp_user_emb\")(user_input)\n",
    "    mlp_item_emb = Embedding(output_dim=int(dense_layers[0] / 2), \n",
    "                             input_dim=num_items, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                             embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"mlp_user_item\")(item_input)\n",
    "    \n",
    "    # Matrix factorization\n",
    "    mf_user_vecs = Reshape([dims])(mf_user_emb)\n",
    "    mf_item_vecs = Reshape([dims])(mf_item_emb)\n",
    "    \n",
    "    mf_vec = multiply([mf_user_vecs, mf_item_vecs])\n",
    "    \n",
    "    #MLP\n",
    "    mlp_vec = Concatenate()([mlp_user_emb, mlp_item_emb])\n",
    "    mlp_vector = Flatten()(mlp_vec)\n",
    "    \n",
    "    for num_nodes in dense_layers:\n",
    "        l = Dense(num_nodes, activation=\"relu\")\n",
    "        mlp_vector = l(mlp_vector)\n",
    "    \n",
    "    y = Concatenate()([mf_vec, mlp_vector])\n",
    "    y = Dense(1, activation=\"sigmoid\", name=\"pred\")(y)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input], outputs=y)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.01),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_neumf = get_model_neumf(num_users, num_items, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a520573",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = df_train.iloc[:, 0].values.reshape((-1,1))\n",
    "item_input = df_train.iloc[:, 7].values.reshape((-1,1))\n",
    "labels = df_train.iloc[:, 11].values.reshape((-1,1))\n",
    "print(user_input.shape, item_input.shape, labels.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e005fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = df_test.user_id.values\n",
    "test_items = df_test.article_id.values\n",
    "test_users, test_items = shuffle(test_users, test_items)\n",
    "test_users, test_items = test_users, test_items\n",
    "test_set = zip(test_users[:100], test_items[:100])\n",
    "hits_ten,hits_five,ndcgs_ten,ndcgs_five = [], [], [], []\n",
    "h_ten, h_five, n_ten, n_five = [], [], [], []\n",
    "for (u,i) in tqdm(test_set):\n",
    "    users = np.array([u]*100)\n",
    "    categories = np.tile(np.array(article_to_category[i]), 100).reshape(-1,1)\n",
    "    \n",
    "    predictions = model_neumf_one_feat.predict([users, items,categories])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [items[k] for k in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    h_ten.append(getHitRatio(top_ten_items, i))\n",
    "    h_five.append(getHitRatio(top_ten_items[:5], i))\n",
    "    n_ten.append(getNDCG(top_ten_items, i))\n",
    "    n_five.append(getNDCG(top_ten_items[:5], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2198d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits, ndcgs, hits_five, ndcgs_five = evalaute_model_neumf( model_neumf, df_test, userid_to_true_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd7b5b",
   "metadata": {},
   "source": [
    "# NeuMF 1 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381bbeeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ca2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(merged[\"user_id\"].unique())\n",
    "num_items = len(merged[\"article_id\"].unique())\n",
    "num_categories = len(merged[\"category_int\"].unique()) \n",
    "num_sub_categories = len(merged[\"subcategory_int\"].unique())\n",
    "\n",
    "dims = 20\n",
    "def get_model_neumfonefeat(num_users, num_items, dims, dense_layers=[128, 64, 32, 8]):\n",
    "    user_input = Input(shape=(1,), name=\"user\")\n",
    "    item_input = Input(shape=(1,), name=\"item\")\n",
    "    \n",
    "    mf_user_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=num_users, \n",
    "                            input_length=1, \n",
    "                            embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_user_emb\")(user_input)\n",
    "    mf_item_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=num_items, \n",
    "                            input_length=1, \n",
    "                            embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_item_emb\")(item_input)\n",
    "    \n",
    "    num_layers = len(dense_layers)\n",
    "    mlp_user_emb = Embedding(output_dim=int(dense_layers[0] / 2), \n",
    "                             input_dim=num_users, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                             embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"mlp_user_emb\")(user_input)\n",
    "    mlp_item_emb = Embedding(output_dim=int(dense_layers[0] / 2), \n",
    "                             input_dim=num_items, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"mlp_user_item\")(item_input)\n",
    "    \n",
    "    # Matrix factorization\n",
    "    mf_user_vecs = Reshape([dims])(mf_user_emb)\n",
    "    mf_item_vecs = Reshape([dims])(mf_item_emb)\n",
    "    \n",
    "    mf_vec = multiply([mf_user_vecs, mf_item_vecs])\n",
    "    \n",
    "    #MLP\n",
    "    category_input = Input(shape=(1,), name=\"category_input\")\n",
    "    \n",
    "    item_category_emb = Embedding(input_dim=num_categories, output_dim=int(dense_layers[0] / 2), name=\"category_emd\", embeddings_regularizer=regularizers.l2(0.001))(category_input)\n",
    "\n",
    "    item_category_flatten = Flatten()(item_category_emb)\n",
    "    user_flatten = Flatten()(mlp_user_emb)\n",
    "    item_flatten = Flatten()(mlp_item_emb)\n",
    "    \n",
    "    \n",
    "    wide_features = Concatenate()([item_category_flatten,user_flatten, item_flatten])\n",
    "    mlp_vector = Flatten()(wide_features)\n",
    "    for num_dense in dense_layers:\n",
    "        l = Dense(num_dense, activation=\"relu\")\n",
    "        mlp_vector = l(mlp_vector)\n",
    "        mlp_vector = Dropout(0.2)(mlp_vector)\n",
    "    \n",
    "\n",
    "    \n",
    "    mlp_vec = Concatenate()([mlp_user_emb, mlp_item_emb])\n",
    "    mlp_vector = Flatten()(mlp_vec)\n",
    "    \n",
    "    y = Concatenate()([mf_vec, mlp_vector])\n",
    "    y = Dense(1, activation=\"sigmoid\", name=\"pred\")(y)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input,category_input], outputs=y)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.01),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_neumf_one_feat = get_model_neumfonefeat(num_users, num_items, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af47c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Training ########\n",
    "user_input = df_train.user_id.values\n",
    "articles = df_train.article_id.values\n",
    "category = df_train.article_category.values\n",
    "labels = df_train.labels.values\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    hist = model_neumf_one_feat.fit([user_input,articles,category], labels, validation_split=0.1, epochs=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b531c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = df_test.user_id.values\n",
    "test_items = df_test.article_id.values\n",
    "test_users, test_items = shuffle(test_users, test_items)\n",
    "test_users, test_items = test_users, test_items\n",
    "test_set = zip(test_users[:100], test_items[:100])\n",
    "hits_ten,hits_five,ndcgs_ten,ndcgs_five = [], [], [], []\n",
    "h_ten, h_five, n_ten, n_five = [], [], [], []\n",
    "for (u,i) in tqdm(test_set):\n",
    "    not_interacted_items = get_not_interacted(u, merged)\n",
    "    users = np.array([u]*100)\n",
    "    items = np.array([i]+not_interacted_items)\n",
    "    categories = np.tile(np.array(article_to_category[i]), 100).reshape(-1,1)\n",
    "    \n",
    "    predictions = model_neumf_one_feat.predict([users, items,categories])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [items[k] for k in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    h_ten.append(getHitRatio(top_ten_items, i))\n",
    "    h_five.append(getHitRatio(top_ten_items[:5], i))\n",
    "    n_ten.append(getNDCG(top_ten_items, i))\n",
    "    n_five.append(getNDCG(top_ten_items[:5], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(h_ten))\n",
    "print(np.average(h_five))\n",
    "print(np.average(n_ten))\n",
    "print(np.average(n_five))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7a507f",
   "metadata": {},
   "source": [
    "# 4.2 NeuMF 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6722d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(merged[\"user_id\"].unique())\n",
    "num_items = len(merged[\"article_id\"].unique())\n",
    "num_categories = len(merged[\"category_int\"].unique()) \n",
    "num_sub_categories = len(merged[\"subcategory_int\"].unique())\n",
    "\n",
    "dims = 20\n",
    "def get_model_neumftwofeat(num_users, num_items, dims, dense_layers=[128, 64, 32, 8]):\n",
    "    user_input = Input(shape=(1,), name=\"user\")\n",
    "    item_input = Input(shape=(1,), name=\"item\")\n",
    "    \n",
    "    mf_user_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=num_users, \n",
    "                            input_length=1, \n",
    "                            embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_user_emb\")(user_input)\n",
    "    mf_item_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=num_items, \n",
    "                            input_length=1, \n",
    "                            embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_item_emb\")(item_input)\n",
    "    \n",
    "    num_layers = len(dense_layers)\n",
    "    mlp_user_emb = Embedding(output_dim=int(dense_layers[0] / 2), \n",
    "                             input_dim=num_users, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                             embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"mlp_user_emb\")(user_input)\n",
    "    mlp_item_emb = Embedding(output_dim=int(dense_layers[0] / 2), \n",
    "                             input_dim=num_items, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"mlp_user_item\")(item_input)\n",
    "    \n",
    "    # Matrix factorization\n",
    "    mf_user_vecs = Reshape([dims])(mf_user_emb)\n",
    "    mf_item_vecs = Reshape([dims])(mf_item_emb)\n",
    "    \n",
    "    mf_vec = multiply([mf_user_vecs, mf_item_vecs])\n",
    "    \n",
    "    #MLP\n",
    "    category_input = Input(shape=(1,), name=\"category_input\")\n",
    "    sub_category_input = Input(shape=(1,), name=\"subcategory_input\")\n",
    "\n",
    "    \n",
    "    item_category_emb = Embedding(input_dim=num_categories, output_dim=int(dense_layers[0] / 2), name=\"category_emd\", embeddings_regularizer=regularizers.l2(0.001))(category_input)\n",
    "    item_subcategory_emb = Embedding(input_dim=num_sub_categories, output_dim=int(dense_layers[0] / 2),embeddings_regularizer=regularizers.l2(0.001), name=\"subcat_emb\")(sub_category_input)\n",
    "\n",
    "    \n",
    "    \n",
    "    item_category_flatten = Flatten()(item_category_emb)\n",
    "    item_subcategory_flatten = Flatten()(item_subcategory_emb)\n",
    "\n",
    "    user_flatten = Flatten()(mlp_user_emb)\n",
    "    item_flatten = Flatten()(mlp_item_emb)\n",
    "    \n",
    "    \n",
    "    wide_features = Concatenate()([item_category_flatten,user_flatten, item_flatten, item_subcategory_flatten])\n",
    "    mlp_vector = Flatten()(wide_features)\n",
    "    for num_dense in dense_layers:\n",
    "        l = Dense(num_dense, activation=\"relu\")\n",
    "        mlp_vector = l(mlp_vector)\n",
    "        mlp_vector = Dropout(0.2)(mlp_vector)\n",
    "    \n",
    "\n",
    "    \n",
    "    mlp_vec = Concatenate()([mlp_user_emb, mlp_item_emb])\n",
    "    mlp_vector = Flatten()(mlp_vec)\n",
    "    \n",
    "    y = Concatenate()([mf_vec, mlp_vector])\n",
    "    y = Dense(1, activation=\"sigmoid\", name=\"pred\")(y)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input,category_input, sub_category_input], outputs=y)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.01),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_neumf_two_feat = get_model_neumftwofeat(num_users, num_items, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Training ########\n",
    "user_input = df_train.user_id.values\n",
    "articles = df_train.article_id.values\n",
    "category = df_train.article_category.values\n",
    "sub_cat = df_train.article_sub_category.values\n",
    "labels = df_train.labels.values\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    hist = model_neumf_two_feat.fit([user_input,articles,category,sub_cat], labels, validation_split=0.1, epochs=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2047e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = df_test.user_id.values\n",
    "test_items = df_test.article_id.values\n",
    "test_users, test_items = shuffle(test_users, test_items)\n",
    "test_users, test_items = test_users, test_items\n",
    "test_set = zip(test_users[:100], test_items[:100])\n",
    "hits_ten,hits_five,ndcgs_ten,ndcgs_five = [], [], [], []\n",
    "h_ten, h_five, n_ten, n_five = [], [], [], []\n",
    "for (u,i) in tqdm(test_set):\n",
    "    not_interacted_items = get_not_interacted(u, merged)\n",
    "    users = np.array([u]*100)\n",
    "    items = np.array([i]+not_interacted_items)\n",
    "    categories = np.tile(np.array(article_to_category[i]), 100).reshape(-1,1)\n",
    "    subcategories = np.tile(np.array(article_to_subcategory[i]), 100).reshape(-1,1)\n",
    "    predictions = model_neumf_two_feat.predict([users, items,categories,subcategories])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [items[k] for k in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    h_ten.append(getHitRatio(top_ten_items, i))\n",
    "    h_five.append(getHitRatio(top_ten_items[:5], i))\n",
    "    n_ten.append(getNDCG(top_ten_items, i))\n",
    "    n_five.append(getNDCG(top_ten_items[:5], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fdf62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(h_ten))\n",
    "print(np.average(h_five))\n",
    "print(np.average(n_ten))\n",
    "print(np.average(n_five))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155df59f",
   "metadata": {},
   "source": [
    "# 4.3 NeuMF 3 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f9b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(merged[\"user_id\"].unique())\n",
    "num_items = len(merged[\"article_id\"].unique())\n",
    "num_categories = len(merged[\"category_int\"].unique()) \n",
    "num_sub_categories = len(merged[\"subcategory_int\"].unique())\n",
    "\n",
    "dims = 20\n",
    "def get_model_neumfthreefeat(num_users, num_items, dims, dense_layers=[128, 64, 32, 8]):\n",
    "    user_input = Input(shape=(1,), name=\"user\")\n",
    "    item_input = Input(shape=(1,), name=\"item\")\n",
    "    \n",
    "    mf_user_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=num_users, \n",
    "                            input_length=1, \n",
    "                            embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_user_emb\")(user_input)\n",
    "    mf_item_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=num_items, \n",
    "                            input_length=1, \n",
    "                            embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_item_emb\")(item_input)\n",
    "    \n",
    "    num_layers = len(dense_layers)\n",
    "    mlp_user_emb = Embedding(output_dim=int(dense_layers[0] / 2), \n",
    "                             input_dim=num_users, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                             embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"mlp_user_emb\")(user_input)\n",
    "    mlp_item_emb = Embedding(output_dim=int(dense_layers[0] / 2), \n",
    "                             input_dim=num_items, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"mlp_user_item\")(item_input)\n",
    "    \n",
    "    # Matrix factorization\n",
    "    mf_user_vecs = Reshape([dims])(mf_user_emb)\n",
    "    mf_item_vecs = Reshape([dims])(mf_item_emb)\n",
    "    \n",
    "    mf_vec = multiply([mf_user_vecs, mf_item_vecs])\n",
    "    \n",
    "    #MLP\n",
    "    category_input = Input(shape=(1,), name=\"category_input\")\n",
    "    sub_category_input = Input(shape=(1,), name=\"subcategory_input\")\n",
    "    title_input = Input(shape=(10,), name=\"title_input\")\n",
    "    \n",
    "    item_category_emb = Embedding(input_dim=num_categories, output_dim=int(dense_layers[0] / 2), name=\"category_emd\", embeddings_regularizer=regularizers.l2(0.001))(category_input)\n",
    "    item_subcategory_emb = Embedding(input_dim=num_sub_categories, output_dim=int(dense_layers[0] / 2),embeddings_regularizer=regularizers.l2(0.001), name=\"subcat_emb\")(sub_category_input)\n",
    "    title_emb = Embedding(input_dim=num_words_title, output_dim=int(dense_layers[0] / 2),embeddings_regularizer=regularizers.l2(0.001), name=\"subcat_emb\")(title_input)\n",
    "\n",
    "    \n",
    "    \n",
    "    item_category_flatten = Flatten()(item_category_emb)\n",
    "    item_subcategory_flatten = Flatten()(item_subcategory_emb)\n",
    "    title_flatten = Flatten()(title_emb)\n",
    "\n",
    "    user_flatten = Flatten()(mlp_user_emb)\n",
    "    item_flatten = Flatten()(mlp_item_emb)\n",
    "    \n",
    "    \n",
    "    wide_features = Concatenate()([item_category_flatten,user_flatten, item_flatten, item_subcategory_flatten, title_flatten])\n",
    "    mlp_vector = Flatten()(wide_features)\n",
    "    for num_dense in dense_layers:\n",
    "        l = Dense(num_dense, activation=\"relu\")\n",
    "        mlp_vector = l(mlp_vector)\n",
    "        mlp_vector = Dropout(0.2)(mlp_vector)\n",
    "    \n",
    "\n",
    "    \n",
    "    mlp_vec = Concatenate()([mlp_user_emb, mlp_item_emb])\n",
    "    mlp_vector = Flatten()(mlp_vec)\n",
    "    \n",
    "    y = Concatenate()([mf_vec, mlp_vector])\n",
    "    y = Dense(1, activation=\"sigmoid\", name=\"pred\")(y)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input,category_input, sub_category_input, title_input], outputs=y)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.01),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_neumf_three_feat = get_model_neumfthreefeat(num_users, num_items, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea34f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Training ########\n",
    "user_input = df_train.user_id.values\n",
    "articles = df_train.article_id.values\n",
    "category = df_train.article_category.values\n",
    "sub_cat = df_train.article_sub_category.values\n",
    "title = np.array([np.array(t) for t in df_train.titles.values])\n",
    "labels = df_train.labels.values\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    hist = model_neumf_three_feat.fit([user_input,articles,category,sub_cat, title], labels, validation_split=0.1, epochs=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2990dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = df_test.user_id.values\n",
    "test_items = df_test.article_id.values\n",
    "test_users, test_items = shuffle(test_users, test_items)\n",
    "test_users, test_items = test_users, test_items\n",
    "test_set = zip(test_users[:100], test_items[:100])\n",
    "hits_ten,hits_five,ndcgs_ten,ndcgs_five = [], [], [], []\n",
    "h_ten, h_five, n_ten, n_five = [], [], [], []\n",
    "for (u,i) in tqdm(test_set):\n",
    "    not_interacted_items = get_not_interacted(u, merged)\n",
    "    users = np.array([u]*100)\n",
    "    items = np.array([i]+not_interacted_items)\n",
    "    categories = np.tile(np.array(article_to_category[i]), 100).reshape(-1,1)\n",
    "    subcategories = np.tile(np.array(article_to_subcategory[i]), 100).reshape(-1,1)\n",
    "    titles = np.tile(np.array(articleId_to_title[i]), 100).reshape(-1,10)\n",
    "    predictions = model_neumf_three_feat.predict([users, items,categories,subcategories,titles])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [items[k] for k in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    h_ten.append(getHitRatio(top_ten_items, i))\n",
    "    h_five.append(getHitRatio(top_ten_items[:5], i))\n",
    "    n_ten.append(getNDCG(top_ten_items, i))\n",
    "    n_five.append(getNDCG(top_ten_items[:5], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(h_ten))\n",
    "print(np.average(h_five))\n",
    "print(np.average(n_ten))\n",
    "print(np.average(n_five))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003c5d9",
   "metadata": {},
   "source": [
    "# 4.4 NeuMF all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcae375",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(merged[\"user_id\"].unique())\n",
    "num_items = len(merged[\"article_id\"].unique())\n",
    "num_categories = len(merged[\"category_int\"].unique()) \n",
    "num_sub_categories = len(merged[\"subcategory_int\"].unique())\n",
    "\n",
    "dims = 20\n",
    "def get_model_neumffourfeat(num_users, num_items, dims, dense_layers=[128, 64, 32, 8]):\n",
    "    user_input = Input(shape=(1,), name=\"user\")\n",
    "    item_input = Input(shape=(1,), name=\"item\")\n",
    "    \n",
    "    mf_user_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=num_users, \n",
    "                            input_length=1, \n",
    "                            embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_user_emb\")(user_input)\n",
    "    mf_item_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=num_items, \n",
    "                            input_length=1, \n",
    "                            embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_item_emb\")(item_input)\n",
    "    \n",
    "    num_layers = len(dense_layers)\n",
    "    mlp_user_emb = Embedding(output_dim=int(dense_layers[0] / 2), \n",
    "                             input_dim=num_users, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                             embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"mlp_user_emb\")(user_input)\n",
    "    mlp_item_emb = Embedding(output_dim=int(dense_layers[0] / 2), \n",
    "                             input_dim=num_items, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"mlp_user_item\")(item_input)\n",
    "    \n",
    "    # Matrix factorization\n",
    "    mf_user_vecs = Reshape([dims])(mf_user_emb)\n",
    "    mf_item_vecs = Reshape([dims])(mf_item_emb)\n",
    "    \n",
    "    mf_vec = multiply([mf_user_vecs, mf_item_vecs])\n",
    "    \n",
    "    #MLP\n",
    "    category_input = Input(shape=(1,), name=\"category_input\")\n",
    "    sub_category_input = Input(shape=(1,), name=\"subcategory_input\")\n",
    "    title_input = Input(shape=(10,), name=\"title_input\")\n",
    "    abstract_input = Input(shape=(10,), name=\"abstract\")\n",
    "    \n",
    "    item_category_emb = Embedding(input_dim=num_categories, output_dim=int(dense_layers[0] / 2), name=\"category_emd\", embeddings_regularizer=regularizers.l2(0.001))(category_input)\n",
    "    item_subcategory_emb = Embedding(input_dim=num_sub_categories, output_dim=int(dense_layers[0] / 2),embeddings_regularizer=regularizers.l2(0.001), name=\"subcat_emb\")(sub_category_input)\n",
    "    title_emb = Embedding(input_dim=num_words_title, output_dim=int(dense_layers[0] / 2),embeddings_regularizer=regularizers.l2(0.001), name=\"subcat_emb\")(title_input)\n",
    "    abstract_emb = Embedding(input_dim=num_words_abstract, output_dim=int(dense_layers[0] / 2),embeddings_regularizer=regularizers.l2(0.001), name=\"subcat_emb\")(abstract_input)\n",
    "\n",
    "    \n",
    "    \n",
    "    item_category_flatten = Flatten()(item_category_emb)\n",
    "    item_subcategory_flatten = Flatten()(item_subcategory_emb)\n",
    "    title_flatten = Flatten()(title_emb)\n",
    "    abs_flatten = Flatten()(abstract_emb)\n",
    "\n",
    "    user_flatten = Flatten()(mlp_user_emb)\n",
    "    item_flatten = Flatten()(mlp_item_emb)\n",
    "    \n",
    "    \n",
    "    wide_features = Concatenate()([item_category_flatten,\n",
    "                                   user_flatten, \n",
    "                                   item_flatten, \n",
    "                                   item_subcategory_flatten, \n",
    "                                   title_flatten, \n",
    "                                   abs_flatten])\n",
    "    mlp_vector = Flatten()(wide_features)\n",
    "    for num_dense in dense_layers:\n",
    "        l = Dense(num_dense, activation=\"relu\")\n",
    "        mlp_vector = l(mlp_vector)\n",
    "        mlp_vector = Dropout(0.2)(mlp_vector)\n",
    "    \n",
    "\n",
    "    \n",
    "    mlp_vec = Concatenate()([mlp_user_emb, mlp_item_emb])\n",
    "    mlp_vector = Flatten()(mlp_vec)\n",
    "    \n",
    "    y = Concatenate()([mf_vec, mlp_vector])\n",
    "    y = Dense(1, activation=\"sigmoid\", name=\"pred\")(y)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input,category_input, sub_category_input, title_input, abstract_input], outputs=y)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.01),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_neumf_four_feat = get_model_neumffourfeat(num_users, num_items, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58646d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Training ########\n",
    "user_input = df_train.user_id.values\n",
    "articles = df_train.article_id.values\n",
    "category = df_train.article_category.values\n",
    "sub_cat = df_train.article_sub_category.values\n",
    "title = np.array([np.array(t) for t in df_train.titles.values])\n",
    "abstract = np.array([np.array(a) for a in df_train.abstract.values])\n",
    "\n",
    "labels = df_train.labels.values\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    hist = model_neumf_four_feat.fit([user_input,articles,category,sub_cat, title,abstract], labels, validation_split=0.1, epochs=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = df_test.user_id.values\n",
    "test_items = df_test.article_id.values\n",
    "test_users, test_items = shuffle(test_users, test_items)\n",
    "test_users, test_items = test_users, test_items\n",
    "test_set = zip(test_users[:100], test_items[:100])\n",
    "hits_ten,hits_five,ndcgs_ten,ndcgs_five = [], [], [], []\n",
    "h_ten, h_five, n_ten, n_five = [], [], [], []\n",
    "for (u,i) in tqdm(test_set):\n",
    "    not_interacted_items = get_not_interacted(u, merged)\n",
    "    users = np.array([u]*100)\n",
    "    items = np.array([i]+not_interacted_items)\n",
    "    categories = np.tile(np.array(article_to_category[i]), 100).reshape(-1,1)\n",
    "    subcategories = np.tile(np.array(article_to_subcategory[i]), 100).reshape(-1,1)\n",
    "    titles = np.tile(np.array(articleId_to_title[i]), 100).reshape(-1,10)\n",
    "    abstracts = np.tile(np.array(article_to_abstract[i]), 100).reshape(-1,10)\n",
    "    predictions = model_neumf_four_feat.predict([users, items,categories,subcategories,titles,abstracts])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [items[k] for k in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    h_ten.append(getHitRatio(top_ten_items, i))\n",
    "    h_five.append(getHitRatio(top_ten_items[:5], i))\n",
    "    n_ten.append(getNDCG(top_ten_items, i))\n",
    "    n_five.append(getNDCG(top_ten_items[:5], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f3333",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(h_ten))\n",
    "print(np.average(h_five))\n",
    "print(np.average(n_ten))\n",
    "print(np.average(n_five))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7248107b",
   "metadata": {},
   "source": [
    "# 5.1 ENSUS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c5e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.autograph.experimental.do_not_convert\n",
    "def get_model(num_users, num_items, dims,num_categories,num_sub_categories, dense_layers=[128, 64, 32, 8]):\n",
    "    #User features\n",
    "    user_history = Input(shape=(10,), name=\"user\")\n",
    "    user_profile_input = Input(shape=(6,), name=\"profile\")\n",
    "    #item features\n",
    "    item_input = Input(shape=(1,), name=\"item\")\n",
    "    item_category = Input(shape=(1,), name=\"category\")\n",
    "    item_subcategory = Input(shape=(1,), name=\"subcategory\")\n",
    "    \n",
    "    # User emb\n",
    "    click_history_emb = Embedding(output_dim=dims, \n",
    "                                  input_dim=num_items+1, \n",
    "                                  input_length=10, \n",
    "                                  embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),\n",
    "                                  name=\"mf_user_emb\")(user_history)\n",
    "    profile_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=num_sub_categories,\n",
    "                            input_length=6, \n",
    "                            embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_profile_emb\")(user_profile_input)\n",
    "    \n",
    "    # Item emb\n",
    "    item_emb = Embedding(output_dim=dims, \n",
    "                         input_dim=num_items+1, \n",
    "                         input_length=1, \n",
    "                         embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),\n",
    "                         name=\"mf_item_emb\")(item_input)\n",
    "    category_emb = Embedding(output_dim=dims, \n",
    "                             input_dim=num_categories, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"cat_emb\")(item_category)\n",
    "\n",
    "    ### Wide\n",
    "    #wide_history = Flatten()(click_history_emb)\n",
    "    #wide_item = Flatten()(item_input)\n",
    "    wide = Concatenate(axis=1)([click_history_emb, item_emb])\n",
    "    wide = Flatten()(wide)\n",
    "    for n in dense_layers:\n",
    "        l = Dense(n, activation=\"relu\")\n",
    "        wide = l(wide)\n",
    "        d = Dropout(0.7)\n",
    "        wide = d(wide)\n",
    "    y_wide = Dense(2)(wide)\n",
    "    \n",
    "    ### Deep\n",
    "    deep_features = category_emb\n",
    "    x_deep = LSTM(40)(deep_features)\n",
    "    x_deep = Dropout(0.5)(x_deep)\n",
    "    x_deep = BatchNormalization(axis=1)(x_deep)\n",
    "    \n",
    "    print(x_deep.shape)\n",
    "    print(y_wide.shape)\n",
    "    \n",
    "    final = Concatenate()([x_deep, y_wide])\n",
    "    final = BatchNormalization(axis=1)(final)\n",
    "   \n",
    "    y = Dense(1, activation=\"sigmoid\")(final)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_history, item_input, item_category], outputs=y)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.001),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_final = get_model(num_users, num_items, dims, num_categories,num_sub_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c4def",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Training ########\n",
    "user_input = df_train.user_id.values\n",
    "articles = df_train.article_id.values\n",
    "category = df_train.article_category.values\n",
    "click_history = df_train.iloc[:,13:].values\n",
    "labels = df_train.labels.values\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    hist = model_final.fit([click_history,articles,category], labels, validation_split=0.1, epochs=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04009e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = df_test.user_id.values\n",
    "test_items = df_test.article_id.values\n",
    "test_users, test_items = shuffle(test_users, test_items)\n",
    "test_users, test_items = test_users, test_items\n",
    "test_set = zip(test_users[:100], test_items[:100])\n",
    "hits_ten,hits_five,ndcgs_ten,ndcgs_five = [], [], [], []\n",
    "h_ten, h_five, n_ten, n_five = [], [], [], []\n",
    "for (u,i) in tqdm(test_set):\n",
    "    not_interacted_items = get_not_interacted(u, merged)\n",
    "    users = np.array([u]*100)\n",
    "    items = np.array([i]+not_interacted_items)\n",
    "    categories = np.tile(np.array(article_to_category[i]), 100).reshape(-1,1)\n",
    "    click_history = np.tile(userid_to_article_history[u], 100).reshape(-1, 10)\n",
    "    predictions = model_final.predict([click_history, items,categories])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [items[k] for k in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    h_ten.append(getHitRatio(top_ten_items, i))\n",
    "    h_five.append(getHitRatio(top_ten_items[:5], i))\n",
    "    n_ten.append(getNDCG(top_ten_items, i))\n",
    "    n_five.append(getNDCG(top_ten_items[:5], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ce1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(h_ten))\n",
    "print(np.average(h_five))\n",
    "print(np.average(n_ten))\n",
    "print(np.average(n_five))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a0b86",
   "metadata": {},
   "source": [
    "# 5.2 Ensus features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10484822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.autograph.experimental.do_not_convert\n",
    "def get_model(num_users, num_items, dims,num_categories,num_sub_categories, dense_layers=[128, 64, 32, 8]):\n",
    "    #User features\n",
    "    user_history = Input(shape=(10,), name=\"user\")\n",
    "    user_profile_input = Input(shape=(6,), name=\"profile\")\n",
    "    #item features\n",
    "    item_input = Input(shape=(1,), name=\"item\")\n",
    "    item_category = Input(shape=(1,), name=\"category\")\n",
    "    item_subcategory = Input(shape=(1,), name=\"subcategory\")\n",
    "    \n",
    "    # User emb\n",
    "    click_history_emb = Embedding(output_dim=dims, \n",
    "                                  input_dim=num_items+1, \n",
    "                                  input_length=10, \n",
    "                                  embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),\n",
    "                                  name=\"mf_user_emb\")(user_history)\n",
    "    profile_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=num_sub_categories, \n",
    "                            input_length=6,\n",
    "                            embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_profile_emb\")(user_profile_input)\n",
    "    \n",
    "    # Item emb\n",
    "    item_emb = Embedding(output_dim=dims, \n",
    "                         input_dim=num_items+1, \n",
    "                         input_length=1, \n",
    "                         embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),\n",
    "                         name=\"mf_item_emb\")(item_input)\n",
    "    category_emb = Embedding(output_dim=dims, \n",
    "                             input_dim=num_categories, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"cat_emb\")(item_category)\n",
    "    subcategory_emb = Embedding(output_dim=dims, \n",
    "                                input_dim=num_sub_categories, \n",
    "                                input_length=1, \n",
    "                                embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),\n",
    "                                name=\"subcat_emb\")(item_subcategory)\n",
    "\n",
    "    ### Wide\n",
    "    #wide_history = Flatten()(click_history_emb)\n",
    "    #wide_item = Flatten()(item_input)\n",
    "    wide = Concatenate(axis=1)([click_history_emb, item_emb])\n",
    "    wide = Flatten()(wide)\n",
    "    for n in dense_layers:\n",
    "        l = Dense(n, activation=\"relu\")\n",
    "        wide = l(wide)\n",
    "        d = Dropout(0.7)\n",
    "        wide = d(wide)\n",
    "    y_wide = Dense(2)(wide)\n",
    "    \n",
    "    ### Deep\n",
    "    deep_features = Concatenate(axis=1)([category_emb, subcategory_emb])\n",
    "    x_deep = LSTM(40)(deep_features)\n",
    "    \n",
    "    print(x_deep.shape)\n",
    "    print(y_wide.shape)\n",
    "    \n",
    "    final = Concatenate()([x_deep, y_wide])\n",
    "    final = BatchNormalization(axis=1)(final)\n",
    "   \n",
    "    y = Dense(1, activation=\"sigmoid\")(final)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_history, item_input, item_category,item_subcategory], outputs=y)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.001),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_final = get_model(num_users, num_items, dims, num_categories,num_sub_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b158749",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Training ########\n",
    "user_input = df_train.user_id.values\n",
    "articles = df_train.article_id.values\n",
    "category = df_train.article_category.values\n",
    "subcategory = df_train.article_sub_category.values\n",
    "click_history = df_train.iloc[:,13:].values\n",
    "labels = df_train.labels.values\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    hist = model_final.fit([click_history,articles,category,subcategory], labels, validation_split=0.1, epochs=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73debfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = df_test.user_id.values\n",
    "test_items = df_test.article_id.values\n",
    "test_users, test_items = shuffle(test_users, test_items)\n",
    "test_users, test_items = test_users, test_items\n",
    "test_set = zip(test_users[:100], test_items[:100])\n",
    "hits_ten,hits_five,ndcgs_ten,ndcgs_five = [], [], [], []\n",
    "h_ten, h_five, n_ten, n_five = [], [], [], []\n",
    "for (u,i) in tqdm(test_set):\n",
    "    not_interacted_items = get_not_interacted(u, merged)\n",
    "    users = np.array([u]*100)\n",
    "    items = np.array([i]+not_interacted_items)\n",
    "    categories = np.tile(np.array(article_to_category[i]), 100).reshape(-1,1)\n",
    "    subcategories = np.tile(np.array(article_to_subcategory[i]), 100).reshape(-1,1)\n",
    "\n",
    "    click_history = np.tile(userid_to_article_history[u], 100).reshape(-1, 10)\n",
    "    predictions = model_final.predict([click_history, items,categories,subcategories])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [items[k] for k in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    h_ten.append(getHitRatio(top_ten_items, i))\n",
    "    h_five.append(getHitRatio(top_ten_items[:5], i))\n",
    "    n_ten.append(getNDCG(top_ten_items, i))\n",
    "    n_five.append(getNDCG(top_ten_items[:5], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(h_ten))\n",
    "print(np.average(h_five))\n",
    "print(np.average(n_ten))\n",
    "print(np.average(n_five))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f36139",
   "metadata": {},
   "source": [
    "# 5. 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b61e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.autograph.experimental.do_not_convert\n",
    "def get_model(num_users, num_items, dims,num_categories,num_sub_categories, dense_layers=[128, 64, 32, 8]):\n",
    "    #User features\n",
    "    user_history = Input(shape=(10,), name=\"user\")\n",
    "    user_profile_input = Input(shape=(6,), name=\"profile\")\n",
    "    #item features\n",
    "    item_input = Input(shape=(1,), name=\"item\")\n",
    "    item_category = Input(shape=(1,), name=\"category\")\n",
    "    item_subcategory = Input(shape=(1,), name=\"subcategory\")\n",
    "    \n",
    "    # User emb\n",
    "    click_history_emb = Embedding(output_dim=dims, \n",
    "                                  input_dim=num_items+1, \n",
    "                                  input_length=10, \n",
    "                                  embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),\n",
    "                                  name=\"mf_user_emb\")(user_history)\n",
    "    profile_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=num_sub_categories, \n",
    "                            input_length=6, \n",
    "                            embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_profile_emb\")(user_profile_input)\n",
    "    \n",
    "    # Item emb\n",
    "    item_emb = Embedding(output_dim=dims, \n",
    "                         input_dim=num_items+1, \n",
    "                         input_length=1, \n",
    "                         embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),\n",
    "                         name=\"mf_item_emb\")(item_input)\n",
    "    category_emb = Embedding(output_dim=dims, \n",
    "                             input_dim=num_categories, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"cat_emb\")(item_category)\n",
    "    subcategory_emb = Embedding(output_dim=dims, \n",
    "                                embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),\n",
    "                                input_dim=num_sub_categories, input_length=1, name=\"subcat_emb\")(item_subcategory)\n",
    "\n",
    "    ### Wide\n",
    "    #wide_history = Flatten()(click_history_emb)\n",
    "    #wide_item = Flatten()(item_input)\n",
    "    wide = Concatenate(axis=1)([click_history_emb, item_emb])\n",
    "    wide = Flatten()(wide)\n",
    "    for n in dense_layers:\n",
    "        l = Dense(n, activation=\"relu\")\n",
    "        wide = l(wide)\n",
    "        d = Dropout(0.7)\n",
    "        wide = d(wide)\n",
    "    y_wide = Dense(2)(wide)\n",
    "    \n",
    "    ### Deep\n",
    "    deep_features = Concatenate(axis=1)([category_emb, subcategory_emb,profile_emb])\n",
    "    x_deep = LSTM(40)(deep_features)\n",
    "    \n",
    "    print(x_deep.shape)\n",
    "    print(y_wide.shape)\n",
    "    \n",
    "    final = Concatenate()([x_deep, y_wide])\n",
    "    final = BatchNormalization(axis=1)(final)\n",
    "   \n",
    "    y = Dense(1, activation=\"sigmoid\")(final)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_history, item_input, item_category,item_subcategory, user_profile_input], outputs=y)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.001),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_final = get_model(num_users, num_items, dims, num_categories,num_sub_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1197e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Training ########\n",
    "user_input = df_train.user_id.values\n",
    "articles = df_train.article_id.values\n",
    "category = df_train.article_category.values\n",
    "subcategory = df_train.article_sub_category.values\n",
    "click_history = df_train.iloc[:,13:].values\n",
    "profile = df_train.iloc[:,1:7].values\n",
    "labels = df_train.labels.values\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    hist = model_final.fit([click_history,articles,category,subcategory,profile], labels, validation_split=0.1, epochs=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbc934",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = df_test.user_id.values\n",
    "test_items = df_test.article_id.values\n",
    "test_users, test_items = shuffle(test_users, test_items)\n",
    "test_users, test_items = test_users, test_items\n",
    "test_set = zip(test_users[:100], test_items[:100])\n",
    "hits_ten,hits_five,ndcgs_ten,ndcgs_five = [], [], [], []\n",
    "h_ten, h_five, n_ten, n_five = [], [], [], []\n",
    "for (u,i) in tqdm(test_set):\n",
    "    not_interacted_items = get_not_interacted(u, merged)\n",
    "    users = np.array([u]*100)\n",
    "    items = np.array([i]+not_interacted_items)\n",
    "    categories = np.tile(np.array(article_to_category[i]), 100).reshape(-1,1)\n",
    "    subcategories = np.tile(np.array(article_to_subcategory[i]), 100).reshape(-1,1)\n",
    "    profile = np.tile(np.array(userid_to_profile[u]), 100).reshape(-1, 6)\n",
    "    click_history = np.tile(userid_to_article_history[u], 100).reshape(-1, 10)\n",
    "    predictions = model_final.predict([click_history, items,categories,subcategories,profile])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [items[k] for k in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    h_ten.append(getHitRatio(top_ten_items, i))\n",
    "    h_five.append(getHitRatio(top_ten_items[:5], i))\n",
    "    n_ten.append(getNDCG(top_ten_items, i))\n",
    "    n_five.append(getNDCG(top_ten_items[:5], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(h_ten))\n",
    "print(np.average(h_five))\n",
    "print(np.average(n_ten))\n",
    "print(np.average(n_five))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a07a07",
   "metadata": {},
   "source": [
    "# 5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad9388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.autograph.experimental.do_not_convert\n",
    "def get_model(num_users, num_items, dims,num_categories,num_sub_categories, dense_layers=[128, 64, 32, 8]):\n",
    "    #User features\n",
    "    user_history = Input(shape=(10,), name=\"user\")\n",
    "    user_profile_input = Input(shape=(6,), name=\"profile\")\n",
    "    #item features\n",
    "    item_input = Input(shape=(1,), name=\"item\")\n",
    "    item_category = Input(shape=(1,), name=\"category\")\n",
    "    item_subcategory = Input(shape=(1,), name=\"subcategory\")\n",
    "    item_title = Input(shape=(10,), name=\"title\")\n",
    "    \n",
    "    # User emb\n",
    "    click_history_emb = Embedding(output_dim=dims,embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001), input_dim=num_items+1, input_length=10, name=\"mf_user_emb\")(user_history)\n",
    "    profile_emb = Embedding(output_dim=dims, embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),input_dim=num_sub_categories, input_length=6, name=\"mf_profile_emb\")(user_profile_input)\n",
    "    \n",
    "    # Item emb\n",
    "    item_emb = Embedding(output_dim=dims, embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),input_dim=num_items+1, input_length=1, name=\"mf_item_emb\")(item_input)\n",
    "    category_emb = Embedding(output_dim=dims,embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001), input_dim=num_categories, input_length=1, name=\"cat_emb\")(item_category)\n",
    "    subcategory_emb = Embedding(output_dim=dims,embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001), input_dim=num_sub_categories, input_length=1, name=\"subcat_emb\")(item_subcategory)\n",
    "    title_emb = Embedding(output_dim=dims, embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),input_dim=num_words_title, input_length=1, name=\"title_em\")(item_title)\n",
    "\n",
    "    ### Wide\n",
    "    #wide_history = Flatten()(click_history_emb)\n",
    "    #wide_item = Flatten()(item_input)\n",
    "    wide = Concatenate(axis=1)([click_history_emb, item_emb, title_emb])\n",
    "    wide = Flatten()(wide)\n",
    "    for n in dense_layers:\n",
    "        l = Dense(n, activation=\"relu\")\n",
    "        wide = l(wide)\n",
    "        d = Dropout(0.7)\n",
    "        wide = d(wide)\n",
    "    y_wide = Dense(2)(wide)\n",
    "    \n",
    "    ### Deep\n",
    "    deep_features = Concatenate(axis=1)([category_emb, subcategory_emb,profile_emb])\n",
    "    x_deep = LSTM(40)(deep_features)\n",
    "    \n",
    "    print(x_deep.shape)\n",
    "    print(y_wide.shape)\n",
    "    \n",
    "    final = Concatenate()([x_deep, y_wide])\n",
    "    final = BatchNormalization(axis=1)(final)\n",
    "   \n",
    "    y = Dense(1, activation=\"sigmoid\")(final)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_history, item_input, item_category,item_subcategory, user_profile_input,item_title], outputs=y)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.001),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_final = get_model(num_users, num_items, dims, num_categories,num_sub_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d85a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Training ########\n",
    "user_input = df_train.user_id.values\n",
    "articles = df_train.article_id.values\n",
    "category = df_train.article_category.values\n",
    "subcategory = df_train.article_sub_category.values\n",
    "click_history = df_train.iloc[:,13:].values\n",
    "profile = df_train.iloc[:,1:7].values\n",
    "labels = df_train.labels.values\n",
    "\n",
    "titles = np.array([np.array(t) for t in df_train.titles.values])\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    hist = model_final.fit([click_history,articles,category,subcategory,profile, titles], labels, validation_split=0.1, epochs=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54108086",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = df_test.user_id.values\n",
    "test_items = df_test.article_id.values\n",
    "test_users, test_items = shuffle(test_users, test_items)\n",
    "test_users, test_items = test_users, test_items\n",
    "test_set = zip(test_users[:100], test_items[:100])\n",
    "hits_ten,hits_five,ndcgs_ten,ndcgs_five = [], [], [], []\n",
    "h_ten, h_five, n_ten, n_five = [], [], [], []\n",
    "for (u,i) in tqdm(test_set):\n",
    "    not_interacted_items = get_not_interacted(u, merged)\n",
    "    users = np.array([u]*100)\n",
    "    items = np.array([i]+not_interacted_items)\n",
    "    categories = np.tile(np.array(article_to_category[i]), 100).reshape(-1,1)\n",
    "    subcategories = np.tile(np.array(article_to_subcategory[i]), 100).reshape(-1,1)\n",
    "    profile = np.tile(np.array(userid_to_profile[u]), 100).reshape(-1, 6)\n",
    "    click_history = np.tile(userid_to_article_history[u], 100).reshape(-1, 10)\n",
    "    titles = np.tile(np.array(articleId_to_title[i]), 100).reshape(-1,10)\n",
    "\n",
    "    predictions = model_final.predict([click_history, items,categories,subcategories,profile,titles])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [items[k] for k in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    h_ten.append(getHitRatio(top_ten_items, i))\n",
    "    h_five.append(getHitRatio(top_ten_items[:5], i))\n",
    "    n_ten.append(getNDCG(top_ten_items, i))\n",
    "    n_five.append(getNDCG(top_ten_items[:5], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66124572",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(h_ten))\n",
    "print(np.average(h_five))\n",
    "print(np.average(n_ten))\n",
    "print(np.average(n_five))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b1169",
   "metadata": {},
   "source": [
    "# 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f28e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.autograph.experimental.do_not_convert\n",
    "def get_model(num_users, num_items, dims,num_categories,num_sub_categories, dense_layers=[128, 64, 32, 8]):\n",
    "    #User features\n",
    "    user_history = Input(shape=(10,), name=\"user\")\n",
    "    user_profile_input = Input(shape=(6,), name=\"profile\")\n",
    "    #item features\n",
    "    item_input = Input(shape=(1,), name=\"item\")\n",
    "    item_category = Input(shape=(1,), name=\"category\")\n",
    "    item_subcategory = Input(shape=(1,), name=\"subcategory\")\n",
    "    item_title = Input(shape=(10,), name=\"title\")\n",
    "    abstract = Input(shape=(10,), name=\"abstract\")\n",
    "    \n",
    "    # User emb\n",
    "    click_history_emb = Embedding(output_dim=dims,embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001), input_dim=num_items+1, input_length=10, name=\"mf_user_emb\")(user_history)\n",
    "    profile_emb = Embedding(output_dim=dims, input_dim=num_sub_categories, input_length=6, name=\"mf_profile_emb\")(user_profile_input)\n",
    "    \n",
    "    # Item emb\n",
    "    item_emb = Embedding(output_dim=dims,embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001), input_dim=num_items+1, input_length=1, name=\"mf_item_emb\")(item_input)\n",
    "    category_emb = Embedding(output_dim=dims,embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001), input_dim=num_categories, input_length=1, name=\"cat_emb\")(item_category)\n",
    "    subcategory_emb = Embedding(output_dim=dims,embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001), input_dim=num_sub_categories, input_length=1, name=\"subcat_emb\")(item_subcategory)\n",
    "    title_emb = Embedding(output_dim=dims,embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001), input_dim=num_words_title, input_length=1, name=\"title_em\")(item_title)\n",
    "    abstract_emb = Embedding(output_dim=dims, embeddings_initializer='he_normal', \n",
    "                                embeddings_regularizer=regularizers.l2(0.001),input_dim=num_words_abstract, input_length=1, name=\"abstract_em\")(abstract)\n",
    "\n",
    "    ### Wide\n",
    "    #wide_history = Flatten()(click_history_emb)\n",
    "    #wide_item = Flatten()(item_input)\n",
    "    wide = Concatenate(axis=1)([click_history_emb, item_emb, title_emb])\n",
    "    wide = Flatten()(wide)\n",
    "    for n in dense_layers:\n",
    "        l = Dense(n, activation=\"relu\")\n",
    "        wide = l(wide)\n",
    "        d = Dropout(0.7)\n",
    "        wide = d(wide)\n",
    "    y_wide = Dense(2)(wide)\n",
    "    \n",
    "    ### Deep\n",
    "    deep_features = Concatenate(axis=1)([category_emb, subcategory_emb,profile_emb])\n",
    "    x_deep = LSTM(40)(deep_features)\n",
    "    \n",
    "    print(x_deep.shape)\n",
    "    print(y_wide.shape)\n",
    "    \n",
    "    final = Concatenate()([x_deep, y_wide])\n",
    "    final = BatchNormalization(axis=1)(final)\n",
    "   \n",
    "    y = Dense(1, activation=\"sigmoid\")(final)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_history, item_input, item_category,item_subcategory, user_profile_input,item_title,abstract], outputs=y)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.001),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_final = get_model(num_users, num_items, dims, num_categories,num_sub_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#abstract = np.array([np.array(a) for a in df_train.abstract.values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da8718",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Training ########\n",
    "user_input = df_train.user_id.values\n",
    "articles = df_train.article_id.values\n",
    "category = df_train.article_category.values\n",
    "subcategory = df_train.article_sub_category.values\n",
    "click_history = df_train.iloc[:,13:].values\n",
    "profile = df_train.iloc[:,1:7].values\n",
    "labels = df_train.labels.values\n",
    "\n",
    "titles = np.array([np.array(t) for t in df_train.titles.values])\n",
    "abstract = np.array([np.array(a) for a in df_train.abstract.values])\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    hist = model_final.fit([click_history,articles,category,subcategory,profile, titles,abstract], labels, validation_split=0.1, epochs=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d3728",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = df_test.user_id.values\n",
    "test_items = df_test.article_id.values\n",
    "test_users, test_items = shuffle(test_users, test_items)\n",
    "test_users, test_items = test_users, test_items\n",
    "test_set = zip(test_users[:100], test_items[:100])\n",
    "hits_ten,hits_five,ndcgs_ten,ndcgs_five = [], [], [], []\n",
    "h_ten, h_five, n_ten, n_five = [], [], [], []\n",
    "for (u,i) in tqdm(test_set):\n",
    "    not_interacted_items = get_not_interacted(u, merged)\n",
    "    users = np.array([u]*100)\n",
    "    items = np.array([i]+not_interacted_items)\n",
    "    categories = np.tile(np.array(article_to_category[i]), 100).reshape(-1,1)\n",
    "    subcategories = np.tile(np.array(article_to_subcategory[i]), 100).reshape(-1,1)\n",
    "    profile = np.tile(np.array(userid_to_profile[u]), 100).reshape(-1, 6)\n",
    "    click_history = np.tile(userid_to_article_history[u], 100).reshape(-1, 10)\n",
    "    titles = np.tile(np.array(articleId_to_title[i]), 100).reshape(-1,10)\n",
    "    abstracts = np.tile(np.array(article_to_abstract[i]), 100).reshape(-1,10)\n",
    "\n",
    "    predictions = model_final.predict([click_history, items,categories,subcategories,profile,titles,abstracts])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [items[k] for k in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    h_ten.append(getHitRatio(top_ten_items, i))\n",
    "    h_five.append(getHitRatio(top_ten_items[:5], i))\n",
    "    n_ten.append(getNDCG(top_ten_items, i))\n",
    "    n_five.append(getNDCG(top_ten_items[:5], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(h_ten))\n",
    "print(np.average(h_five))\n",
    "print(np.average(n_ten))\n",
    "print(np.average(n_five))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e3739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
