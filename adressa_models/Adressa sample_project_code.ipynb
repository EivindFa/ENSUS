{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d2e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import ExplicitMF as mf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import scipy\n",
    "#from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input,Flatten, Embedding, Reshape, Multiply, Dropout, Dense, Concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Layer, SpatialDropout1D, GlobalMaxPooling1D, Bidirectional, GRU\n",
    "from tensorflow.keras.layers import Dot, TimeDistributed, BatchNormalization, multiply\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import collections\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a3ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the methods traverse_dir(), load_data() and the dataset are provided by supervisor Peng Liu\n",
    "\n",
    "def traverse_dir(rootDir, level=2):\n",
    "    \n",
    "    dir_list = []\n",
    "    print(\">>>\",rootDir)\n",
    "    for lists in os.listdir(rootDir):\n",
    "        path = os.path.join(rootDir, lists)\n",
    "        if level == 1:\n",
    "            dir_list.append(path)\n",
    "        else:\n",
    "            if os.path.isdir(path):\n",
    "                temp_list = traverse_dir(path, level)\n",
    "                dir_list.extend(temp_list)\n",
    "            else:\n",
    "                dir_list.append(path)\n",
    "    return dir_list\n",
    "\n",
    "def load_data(rootpath, flist):\n",
    "    \"\"\"\n",
    "        Load events from files and convert to dataframe.\n",
    "    \"\"\"\n",
    "    map_lst = []\n",
    "    for fname in flist:\n",
    "        #fname = os.path.join(rootpath, f)\n",
    "        for line in open(fname):\n",
    "            obj = json.loads(line.strip())\n",
    "            if not obj is None:\n",
    "                map_lst.append(obj)\n",
    "    return pd.DataFrame(map_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad7b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath=\"./active1000/\"\n",
    "flist = traverse_dir(fpath)\n",
    "df = load_data(fpath, flist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f03f983",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8afd2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "df = df[df[\"documentId\"].notna()]\n",
    "print(len(df))\n",
    "\n",
    "user_enc = LabelEncoder()\n",
    "article_enc = LabelEncoder()\n",
    "df[\"user_id\"] = user_enc.fit_transform(df[\"userId\"].values)\n",
    "df[\"article_id\"] = article_enc.fit_transform(df[\"documentId\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6682de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68973bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_one_category(text):\n",
    "    \"\"\"\n",
    "    Convert hyphen to underline for the subcategories. So that Tfidf works correctly\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cat = text.split(\"|\")\n",
    "        if len(cat) > 1:\n",
    "            return cat[1]\n",
    "        return cat\n",
    "    except:\n",
    "        return \"null\"\n",
    "#behaviors = clean_title(behaviors)\n",
    "df[\"category_cleaned\"] = df[\"category\"].apply(func = take_one_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_enc = LabelEncoder()\n",
    "df[\"category_int\"] = category_enc.fit_transform(df[\"category_cleaned\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd52672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_userid_to_profile(df):\n",
    "    users = df[\"user_id\"].unique()\n",
    "    userid_to_profile = collections.defaultdict(list)\n",
    "    for user_id in tqdm(users):\n",
    "        user_subcat = df[df[\"user_id\"] == user_id][\"category_int\"].values.tolist()\n",
    "        counter = Counter(user_subcat)\n",
    "        s = sorted(user_subcat, key=lambda x: (counter[x], x), reverse=True)\n",
    "        final_subcategories = []\n",
    "        for elem in s:\n",
    "            if elem not in final_subcategories:\n",
    "                final_subcategories.append(elem)\n",
    "        while len(final_subcategories) < 6:\n",
    "            final_subcategories.append(0)\n",
    "        userid_to_profile[user_id] = final_subcategories[:6]\n",
    "    return userid_to_profile\n",
    "userid_to_profile = get_userid_to_profile(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb41e80",
   "metadata": {},
   "source": [
    "# 2. Train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af029bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "SAMPLE_SIZE = 99\n",
    "NUM_NEGATIVES = 4\n",
    "ALL_ARTICLE_IDS = df[\"article_id\"].unique()\n",
    "NUM_ARTICLES = len(ALL_ARTICLE_IDS)\n",
    "ALL_USERS = df[\"user_id\"].unique()\n",
    "NUM_USERS = len(ALL_USERS)\n",
    "NUM_CATEGORIES = len(df[\"category_int\"].unique())\n",
    "\n",
    "### Global dicts ###\n",
    "    #userid_to_profile\n",
    "    #userid_to_click_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3065bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rank_latest\"] = df.groupby([\"user_id\"])[\"time\"].rank(method=\"first\", ascending=False)\n",
    "\n",
    "train_true = df[df['rank_latest'] != 1]\n",
    "test_true = df[df['rank_latest'] == 1]\n",
    "\n",
    "rating = [1 for i in range(len(train_true))]\n",
    "train_true[\"label\"] = rating\n",
    "\n",
    "train = train_true[[\"user_id\", \"article_id\", \"label\"]]\n",
    "test = test_true[[\"user_id\", \"article_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb32e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_userid_to_click_history(df):\n",
    "    userid_to_article_history = {}\n",
    "    for user_id in tqdm(df[\"user_id\"].unique()):\n",
    "        click_history = df[df[\"user_id\"] == user_id][\"article_id\"].values\n",
    "        if len(click_history) < 10:\n",
    "            while len(click_history) < 10:\n",
    "                click_history = np.append(click_history, 0)\n",
    "        if len(click_history) > 10:\n",
    "            click_history = click_history[:10]\n",
    "        userid_to_article_history[user_id] = click_history\n",
    "    return userid_to_article_history\n",
    "userid_to_click_history = get_userid_to_click_history(train_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e07383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(article_id, df=df):  \n",
    "    return df[df[\"article_id\"] == article_id][\"category_int\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b23b20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554f1fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf1d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items_interacted(user_id, df):\n",
    "    interacted_items = df[df[\"user_id\"]==user_id][\"article_id\"]\n",
    "    return set(interacted_items if type(interacted_items) == pd.Series else [interacted_items])\n",
    "\n",
    "def get_not_interacted(user_id, interactions_df=df):\n",
    "    interacted_items = get_items_interacted(user_id, interactions_df)\n",
    "    all_items = set(df[\"article_id\"])\n",
    "    not_interacted_items = all_items - interacted_items\n",
    "    random.seed(SEED)\n",
    "    not_interacted_items = random.sample(not_interacted_items, SAMPLE_SIZE)\n",
    "    return not_interacted_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf0140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_to_category = df[[\"article_id\", \"category_int\"]].set_index(\"article_id\").to_dict()[\"category_int\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ba02ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd73bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2bdc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(train_df, user_id, article_id):\n",
    "    \"\"\"\n",
    "    Negative sample training instance; for each positive instance, add 4 negative articles\n",
    "    \n",
    "    Return user_ids, news_ids, category_1, category_2, authors_onehotencoded, titles\n",
    "    \"\"\"\n",
    "    \n",
    "    users, articles, categories, click_history, profiles, labels = [], [], [], [], [], []\n",
    "    user_item_set = set(zip(train_df[user_id].values, train_df[article_id].values))\n",
    "    for (u,i) in user_item_set:\n",
    "        for _ in range(NUM_NEGATIVES):\n",
    "            negative_item = np.random.choice(ALL_ARTICLE_IDS)\n",
    "            while (u, negative_item) in user_item_set:\n",
    "                negative_item = np.random.choice(ALL_ARTICLE_IDS)\n",
    "            users.append(u)\n",
    "            articles.append(negative_item)\n",
    "            categories.append(article_to_category[negative_item])\n",
    "            click_history.append(userid_to_click_history[u])\n",
    "            profiles.append(userid_to_profile[u])\n",
    "            labels.append(0)\n",
    "        users.append(u)\n",
    "        articles.append(i)\n",
    "        categories.append(article_to_category[i])\n",
    "        click_history.append(userid_to_click_history[u])\n",
    "        profiles.append(userid_to_profile[u])\n",
    "        labels.append(1)\n",
    "    \n",
    "    users, articles,categories,click_history,profiles, labels = shuffle(users, articles,categories,click_history,profiles, labels, random_state=0)\n",
    "    click_history = np.concatenate(click_history).reshape(-1, 10)\n",
    "    profiles = np.concatenate(profiles).reshape(-1,6)\n",
    "    return users, articles,categories,click_history,profiles, labels\n",
    "\n",
    "train_users, train_articles,train_categories,train_click_history,train_profiles, train_labels = negative_sampling(train, \"user_id\", \"article_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29fd13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(list(zip(train_users, train_articles,train_categories,train_click_history,train_profiles, train_labels)),\n",
    "                       columns=[\"user_id\", \"article_id\", \"category\", \"click_history\", \"user_profile\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38a3c0a",
   "metadata": {},
   "source": [
    "# 4. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01d07c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc18f6",
   "metadata": {},
   "source": [
    "# 4.1 Neumf without features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801bdc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = NUM_USERS\n",
    "num_items = NUM_ARTICLES\n",
    "dims = 20\n",
    "def get_model_neumf(num_users, num_items, dims, dense_layers=[128, 64, 32, 8]):\n",
    "    user_input = Input(shape=(1,), name=\"user\")\n",
    "    item_input = Input(shape=(1,), name=\"item\")\n",
    "    \n",
    "    mf_user_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=num_users, \n",
    "                            input_length=1, \n",
    "                            embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_user_emb\")(user_input)\n",
    "    mf_item_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=num_items, \n",
    "                            input_length=1, \n",
    "                            embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_item_emb\")(item_input)\n",
    "    \n",
    "    num_layers = len(dense_layers)\n",
    "    mlp_user_emb = Embedding(output_dim=int(dense_layers[0] / 2), \n",
    "                             input_dim=num_users, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                             embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"mlp_user_emb\")(user_input)\n",
    "    mlp_item_emb = Embedding(output_dim=int(dense_layers[0] / 2), \n",
    "                             input_dim=num_items, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                             embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"mlp_user_item\")(item_input)\n",
    "    \n",
    "    # Matrix factorization\n",
    "    mf_user_vecs = Reshape([dims])(mf_user_emb)\n",
    "    mf_item_vecs = Reshape([dims])(mf_item_emb)\n",
    "    \n",
    "    mf_vec = multiply([mf_user_vecs, mf_item_vecs])\n",
    "    \n",
    "    #MLP\n",
    "    mlp_vec = Concatenate()([mlp_user_emb, mlp_item_emb])\n",
    "    mlp_vector = Flatten()(mlp_vec)\n",
    "    \n",
    "    for num_nodes in dense_layers:\n",
    "        l = Dense(num_nodes, activation=\"relu\")\n",
    "        mlp_vector = l(mlp_vector)\n",
    "    \n",
    "    y = Concatenate()([mf_vec, mlp_vector])\n",
    "    y = Dense(1, activation=\"sigmoid\", name=\"pred\")(y)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input], outputs=y)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.01),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_neumf = get_model_neumf(num_users, num_items, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64a4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_input, articles_input, labels_input = np.array(train_users).reshape(-1,1), np.array(train_articles).reshape(-1,1), np.array(train_labels).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab4ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_ids = train.index.unique().values\n",
    "\n",
    "#user_input = df_train.iloc[:, 0].values.reshape((-1,1))\n",
    "#profile_input = df_train.iloc[:, 1:6].values\n",
    "#item_input = df_train.iloc[:, 7].values.reshape((-1,1))\n",
    "#labels = df_train.iloc[:, 8].values.reshape((-1,1))\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "hits_list = []\n",
    "ndcg_list = []\n",
    "best_hits = 0\n",
    "best_ndcgs = 0\n",
    "best_hits_five = 0\n",
    "best_ndcgs_five = 0\n",
    "\n",
    "epochs=4\n",
    "for epoch in range(epochs):\n",
    "    hist = model_neumf.fit([users_input, articles_input], labels_input, epochs=1, shuffle=True, verbose=1, batch_size=1024)\n",
    "    \n",
    "    train_loss.append(hist.history[\"loss\"])\n",
    "    train_acc.append(hist.history[\"accuracy\"])\n",
    "    #val_loss.append(hist.history[\"val_loss\"])\n",
    "    #val_acc.append(hist.history[\"val_accuracy\"])\n",
    "    \n",
    "    #hits, ndcgs, hits_five, ndcgs_five = evalaute_model_neumf( model_neumf, df_test, userid_to_true_item)\n",
    "    #hits_list.append(np.average(hits))\n",
    "    #ndcg_list.append(np.average(ndcgs))\n",
    "    \n",
    "    #temp_hits = np.average(hits)\n",
    "    #temp_ndcgs = np.average(ndcgs)\n",
    "    #if (temp_hits > best_hits):\n",
    "    #    best_hits = temp_hits\n",
    "    #    best_ndcgs = temp_ndcgs\n",
    "    #    best_hits_five = np.average(hits_five)\n",
    "    #    best_ndcgs_five = np.average(ndcgs_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9658c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = test.user_id.values\n",
    "test_items = test.article_id.values\n",
    "test_users, test_items = shuffle(test_users, test_items)\n",
    "test_users, test_items = test_users[:100], test_items[:100]\n",
    "test_set = zip(test_users, test_items)\n",
    "hits_ten,hits_five,ndcgs_ten,ndcgs_five = [], [], [], []\n",
    "for (u,i) in tqdm(test_set):\n",
    "    not_interacted_items = get_not_interacted(u)\n",
    "    users = np.array([u]*100)\n",
    "    items = np.array([i]+not_interacted_items)\n",
    "    predictions = model_neumf.predict([users, items])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [items[k] for k in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    hits_ten.append(getHitRatio(top_ten_items, i))\n",
    "    hits_five.append(getHitRatio(top_ten_items[:5], i))\n",
    "    ndcgs_ten.append(getNDCG(top_ten_items, i))\n",
    "    ndcgs_five.append(getNDCG(top_ten_items[:5], i))\n",
    "print(np.average(hits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5ada70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hit @ 10: {:.2f}\".format(np.average(hits_ten)))\n",
    "print(\"ncdgs @ 10: {:.2f}\".format(np.average(hits_five)))\n",
    "print(\"Hit @ 10: {:.2f}\".format(np.average(ndcgs_ten)))\n",
    "print(\"ncdgs @ 10: {:.2f}\".format(np.average(ndcgs_five)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d5a2c4",
   "metadata": {},
   "source": [
    "# 4.2 NCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e750b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_ncf(num_users, num_items, dims, dense_layers=[128, 64, 32, 8]):\n",
    "    user_input = Input(shape=(1,), name=\"user\")\n",
    "    item_input = Input(shape=(1,), name=\"item\")\n",
    "    \n",
    "    user_emb = Embedding(output_dim=dims, \n",
    "                         input_dim=num_users, \n",
    "                         input_length=1, \n",
    "                         embeddings_initializer='he_normal', \n",
    "                         embeddings_regularizer=regularizers.l2(0.001),\n",
    "                         name=\"mf_user_emb\")(user_input)\n",
    "    item_emb = Embedding(output_dim=dims, \n",
    "                         input_dim=num_items, \n",
    "                         input_length=1, \n",
    "                         embeddings_initializer='he_normal', \n",
    "                         embeddings_regularizer=regularizers.l2(0.001),\n",
    "                         name=\"mf_item_emb\")(item_input)\n",
    "    \n",
    "    user_vecs = Reshape([dims])(user_emb)\n",
    "    item_vecs = Reshape([dims])(item_emb)\n",
    "    \n",
    "    y = Dot(1, normalize=False)([user_vecs, item_vecs])\n",
    "    \n",
    "    y = Dense(1, activation=\"sigmoid\")(y)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input], outputs=y)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.01),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_ncf = get_model_ncf(num_users, num_items, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_input, articles_input, labels_input = np.array(train_users).reshape(-1,1), np.array(train_articles).reshape(-1,1), np.array(train_labels).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_ids = train.index.unique().values\n",
    "\n",
    "#user_input = df_train.iloc[:, 0].values.reshape((-1,1))\n",
    "#profile_input = df_train.iloc[:, 1:6].values\n",
    "#item_input = df_train.iloc[:, 7].values.reshape((-1,1))\n",
    "#labels = df_train.iloc[:, 8].values.reshape((-1,1))\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "hits_list = []\n",
    "ndcg_list = []\n",
    "best_hits = 0\n",
    "best_ndcgs = 0\n",
    "best_hits_five = 0\n",
    "best_ndcgs_five = 0\n",
    "\n",
    "epochs=4\n",
    "for epoch in range(epochs):\n",
    "    hist = model_ncf.fit([users_input, articles_input], labels_input, epochs=1, shuffle=True, verbose=1, batch_size=1024)\n",
    "    \n",
    "    train_loss.append(hist.history[\"loss\"])\n",
    "    train_acc.append(hist.history[\"accuracy\"])\n",
    "    #val_loss.append(hist.history[\"val_loss\"])\n",
    "    #val_acc.append(hist.history[\"val_accuracy\"])\n",
    "    \n",
    "    #hits, ndcgs, hits_five, ndcgs_five = evalaute_model_neumf( model_neumf, df_test, userid_to_true_item)\n",
    "    #hits_list.append(np.average(hits))\n",
    "    #ndcg_list.append(np.average(ndcgs))\n",
    "    \n",
    "    #temp_hits = np.average(hits)\n",
    "    #temp_ndcgs = np.average(ndcgs)\n",
    "    #if (temp_hits > best_hits):\n",
    "    #    best_hits = temp_hits\n",
    "    #    best_ndcgs = temp_ndcgs\n",
    "    #    best_hits_five = np.average(hits_five)\n",
    "    #    best_ndcgs_five = np.average(ndcgs_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8aa45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = test.user_id.values\n",
    "test_items = test.article_id.values\n",
    "test_users, test_items = shuffle(test_users, test_items)\n",
    "test_users, test_items = test_users[:100], test_items[:100]\n",
    "test_set = zip(test_users, test_items)\n",
    "hits_ten,hits_five,ndcgs_ten,ndcgs_five = [], [], [], []\n",
    "for (u,i) in tqdm(test_set):\n",
    "    not_interacted_items = get_not_interacted(u)\n",
    "    users = np.array([u]*100)\n",
    "    items = np.array([i]+not_interacted_items)\n",
    "    predictions = model_neumf.predict([users, items])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [items[k] for k in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    hits_ten.append(getHitRatio(top_ten_items, i))\n",
    "    hits_five.append(getHitRatio(top_ten_items[:5], i))\n",
    "    ndcgs_ten.append(getNDCG(top_ten_items, i))\n",
    "    ndcgs_five.append(getNDCG(top_ten_items[:5], i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9849c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hit @ 10: {:.2f}\".format(np.average(hits_ten)))\n",
    "print(\"ncdgs @ 10: {:.2f}\".format(np.average(hits_five)))\n",
    "print(\"Hit @ 10: {:.2f}\".format(np.average(ndcgs_ten)))\n",
    "print(\"ncdgs @ 10: {:.2f}\".format(np.average(ndcgs_five)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465a57f",
   "metadata": {},
   "source": [
    "# 4.3 Popularity based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f1131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_popular_df = pd.DataFrame(df[\"article_id\"].value_counts())\n",
    "most_popular_df = most_popular_df.reset_index()\n",
    "most_popular_df.columns=[\"article_id\", \"counts\"]\n",
    "most_popular_articles = most_popular_df[\"article_id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a22026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_recommender(top_n, user_interactions, most_popular_articles,num_unique_users):\n",
    "    \"\"\"\n",
    "    params: \n",
    "        top_n: number of articles to recommend\n",
    "    \"\"\"\n",
    "    all_article_ids = df[\"article_id\"].unique()\n",
    "    recommendations = {}\n",
    "    for (u,i) in tqdm(user_interactions.items()):\n",
    "        interacted_items = user_interactions[u]\n",
    "        popular_items_not_interacted_with = []\n",
    "        for i in range(10):\n",
    "            counter = i\n",
    "            popular_item = most_popular_articles[i]\n",
    "            while popular_item in interacted_items:\n",
    "                counter += 1\n",
    "                popular_item = most_popular_articles[counter]\n",
    "            popular_items_not_interacted_with.append(popular_item)\n",
    "        recommendations[u] = list(popular_items_not_interacted_with)\n",
    "    return recommendations\n",
    "\n",
    "user_interactions = df_train.groupby(\"user_id\")[\"article_id\"].apply(list).to_dict()\n",
    "num_unique_users = len(df_train[\"user_id\"].unique())\n",
    "recs = popularity_recommender(10, user_interactions, most_popular_articles, num_unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c690274",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = test.user_id.values\n",
    "test_items = test.article_id.values\n",
    "test_users, test_items = test_users[:100], test_items[:100]\n",
    "test_set = zip(test_users, test_items)\n",
    "hits_ten,hits_five,ndcgs_ten,ndcgs_five = [], [], [], []\n",
    "for (u,i) in tqdm(test_set):\n",
    "    top_ten_items = recs[u]\n",
    "    \n",
    "    hits_ten.append(getHitRatio(top_ten_items, i))\n",
    "    hits_five.append(getHitRatio(top_ten_items[:5], i))\n",
    "    ndcgs_ten.append(getNDCG(top_ten_items, i))\n",
    "    ndcgs_five.append(getNDCG(top_ten_items[:5], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e78556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hit @ 10: {:.2f}\".format(np.average(hits_ten)))\n",
    "print(\"ncdgs @ 10: {:.2f}\".format(np.average(hits_five)))\n",
    "print(\"Hit @ 10: {:.2f}\".format(np.average(ndcgs_ten)))\n",
    "print(\"ncdgs @ 10: {:.2f}\".format(np.average(ndcgs_five)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3e90d5",
   "metadata": {},
   "source": [
    "# 4.4 Wide and deep with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33206680",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ARTICLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52389f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_wide(num_users, num_items, dims, dense_layers=[128, 64, 32, 8]):\n",
    "    #### Matrix factorization ####\n",
    "    user_id_input = Input(shape=[1], name=\"user_id\")\n",
    "    item_id_input = Input(shape=[1], name=\"item_id\")\n",
    "    user_embedding = Embedding(input_dim=NUM_USERS, \n",
    "                               output_dim=dims, \n",
    "                               input_length=1, \n",
    "                               embeddings_initializer='he_normal', \n",
    "                               embeddings_regularizer=regularizers.l2(0.001),\n",
    "                               name=\"user_embedding\")(user_id_input)\n",
    "    item_embedding = Embedding(input_dim=NUM_ARTICLES, \n",
    "                               output_dim=dims, \n",
    "                               embeddings_initializer='he_normal', \n",
    "                               embeddings_regularizer=regularizers.l2(0.001),\n",
    "                               name=\"item_embedding\")(item_id_input)\n",
    "    \n",
    "    user_flatten = Flatten()(user_embedding)\n",
    "    item_flatten = Flatten()(item_embedding)\n",
    "    mf_vec = Concatenate()([user_flatten, item_flatten])\n",
    "    \n",
    "    x_deep = Dense(128, activation=\"relu\", kernel_initializer='he_uniform',kernel_regularizer=regularizers.l2(0.001))(mf_vec)\n",
    "    x_deep = Dropout(0.2)(x_deep)\n",
    "    x_deep = Dense(64, activation=\"relu\",\n",
    "                   kernel_initializer='he_uniform', \n",
    "                   kernel_regularizer=regularizers.l2(0.001))(x_deep)\n",
    "    x_deep = Dropout(0.2)(x_deep)\n",
    "    \n",
    "    #### Wide part ####\n",
    "    \n",
    "    user_profile_input = Input(shape=(6,), name=\"user_profile\")\n",
    "    item_category_input = Input(shape=(1,), name=\"category_input\")\n",
    "    \n",
    "    item_category_emb = Embedding(input_dim=NUM_CATEGORIES, output_dim=dims, name=\"category_emd\", embeddings_regularizer=regularizers.l2(0.001))(item_category_input)\n",
    "    user_profile_emb = Embedding(input_dim=NUM_CATEGORIES, output_dim=dims,\n",
    "                                 embeddings_regularizer=regularizers.l2(0.001), name=\"profile_emb\")(user_profile_input)\n",
    "\n",
    "    item_category_flatten = Flatten()(item_category_emb)\n",
    "    user_profile_flatten = Flatten()(user_profile_emb)\n",
    "    \n",
    "    wide_features = Concatenate()([item_category_flatten,  user_profile_flatten])\n",
    "    \n",
    "    x_wide = Dense(128, activation=\"relu\",kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.001))(wide_features)\n",
    "    x_wide = Dropout(0.5)(x_wide)\n",
    "    x_wide = Dense(64, activation=\"relu\",kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.001))(x_wide)\n",
    "    x_wide = Dropout(0.5)(x_wide)\n",
    "    \n",
    "    final = Concatenate()([x_deep,x_wide])\n",
    "    x = Dense(128, kernel_initializer='he_uniform',activation=\"relu\")(final)\n",
    "    x = Dropout(0.5)(x)\n",
    "    y = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_id_input, user_profile_input, item_id_input, item_category_input], outputs=y)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.001),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_wide = get_model_wide(num_users, num_items, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187d3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_input, articles_input, labels_input = np.array(train_users).reshape(-1,1), np.array(train_articles).reshape(-1,1), np.array(train_labels).reshape(-1,1)\n",
    "categories_input = np.array(train_categories).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d395c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_users, train_articles,train_categories,train_click_history,train_profiles, train_labels\n",
    "#user_id_input, user_profile_input, item_id_input, item_category_input\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "hits_list = []\n",
    "ndcg_list = []\n",
    "best_hits = 0\n",
    "best_ndcgs = 0\n",
    "best_hits_five = 0\n",
    "best_ndcgs_five = 0\n",
    "\n",
    "epochs=4\n",
    "for epoch in range(epochs):\n",
    "    hist = model_wide.fit([users_input, train_profiles, articles_input, categories_input], labels_input, epochs=1, shuffle=True, verbose=1, batch_size=1024)\n",
    "    \n",
    "    train_loss.append(hist.history[\"loss\"])\n",
    "    train_acc.append(hist.history[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = test.user_id.values\n",
    "test_items = test.article_id.values\n",
    "test_users, test_items = shuffle(test_users, test_items)\n",
    "test_users, test_items = test_users[:100], test_items[:100]\n",
    "test_set = zip(test_users, test_items)\n",
    "hits_ten,hits_five,ndcgs_ten,ndcgs_five = [], [], [], []\n",
    "for (u,i) in tqdm(test_set):\n",
    "    not_interacted_items = get_not_interacted(u)\n",
    "    users = np.array([u]*100)\n",
    "    items = np.array([i]+not_interacted_items)\n",
    "    profiles = np.tile(np.array(userid_to_profile[u]), 100).reshape(-1, 6)\n",
    "    categories = np.tile(np.array(article_to_category[i]), 100).reshape(-1,1)\n",
    "    \n",
    "    predictions = model_wide.predict([users,profiles, items,categories])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [items[k] for k in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    hits_ten.append(getHitRatio(top_ten_items, i))\n",
    "    hits_five.append(getHitRatio(top_ten_items[:5], i))\n",
    "    ndcgs_ten.append(getNDCG(top_ten_items, i))\n",
    "    ndcgs_five.append(getNDCG(top_ten_items[:5], i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d66021",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hit @ 10: {:.2f}\".format(np.average(hits_ten)))\n",
    "print(\"ncdgs @ 10: {:.2f}\".format(np.average(hits_five)))\n",
    "print(\"Hit @ 10: {:.2f}\".format(np.average(ndcgs_ten)))\n",
    "print(\"ncdgs @ 10: {:.2f}\".format(np.average(ndcgs_five)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dcce0f",
   "metadata": {},
   "source": [
    "# 4.5 NeuMF with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd0a759",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUM_USERS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-46c60375203a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mmodel_neumffeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_neumffeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_USERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_ARTICLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'NUM_USERS' is not defined"
     ]
    }
   ],
   "source": [
    "def get_model_neumffeat(num_users, num_items, dims, dense_layers=[128, 64, 32, 8]):\n",
    "    user_input = Input(shape=(1,), name=\"user\")\n",
    "    item_input = Input(shape=(1,), name=\"item\")\n",
    "    \n",
    "    mf_user_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=NUM_USERS, \n",
    "                            input_length=1, \n",
    "                            embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_user_emb\")(user_input)\n",
    "    mf_item_emb = Embedding(output_dim=dims, \n",
    "                            input_dim=NUM_ARTICLES, \n",
    "                            input_length=1, \n",
    "                            embeddings_initializer='he_normal', \n",
    "                            embeddings_regularizer=regularizers.l2(0.001),\n",
    "                            name=\"mf_item_emb\")(item_input)\n",
    "    \n",
    "    num_layers = len(dense_layers)\n",
    "    mlp_user_emb = Embedding(output_dim=int(dense_layers[0] / 2), \n",
    "                             input_dim=NUM_USERS, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                             embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"mlp_user_emb\")(user_input)\n",
    "    mlp_item_emb = Embedding(output_dim=int(dense_layers[0] / 2), \n",
    "                             input_dim=NUM_ARTICLES, \n",
    "                             input_length=1, \n",
    "                             embeddings_initializer='he_normal', \n",
    "                             embeddings_regularizer=regularizers.l2(0.001),\n",
    "                             name=\"mlp_user_item\")(item_input)\n",
    "    \n",
    "    # Matrix factorization\n",
    "    mf_user_vecs = Reshape([dims])(mf_user_emb)\n",
    "    mf_item_vecs = Reshape([dims])(mf_item_emb)\n",
    "    \n",
    "    mf_vec = multiply([mf_user_vecs, mf_item_vecs])\n",
    "    \n",
    "    #MLP\n",
    "    profile_input = Input(shape=(6,), name=\"user_profile\")\n",
    "    category_input = Input(shape=(1,), name=\"category_input\")\n",
    "    sub_category_input = Input(shape=(1,), name=\"subcategory_input\")\n",
    "    \n",
    "    item_category_emb = Embedding(input_dim=NUM_CATEGORIES, \n",
    "                                  output_dim=int(dense_layers[0] / 2), \n",
    "                                  name=\"category_emd\", \n",
    "                                  embeddings_regularizer=regularizers.l2(0.001))(category_input)\n",
    "    user_profile_emb = Embedding(input_dim=NUM_CATEGORIES, \n",
    "                                 output_dim=int(dense_layers[0] / 2),\n",
    "                                 embeddings_regularizer=regularizers.l2(0.001), \n",
    "                                 name=\"profile_emb\")(profile_input)\n",
    "\n",
    "    item_category_flatten = Flatten()(item_category_emb)\n",
    "    user_profile_flatten = Flatten()(user_profile_emb)\n",
    "    \n",
    "    wide_features = Concatenate()([item_category_flatten,  user_profile_flatten])\n",
    "    mlp_vector = Flatten()(wide_features)\n",
    "    for num_dense in dense_layers:\n",
    "        l = Dense(num_dense, activation=\"relu\")\n",
    "        mlp_vector = l(mlp_vector)\n",
    "        mlp_vector = Dropout(0.2)(mlp_vector)\n",
    "    \n",
    "\n",
    "    \n",
    "    mlp_vec = Concatenate()([mlp_user_emb, mlp_item_emb])\n",
    "    mlp_vector = Flatten()(mlp_vec)\n",
    "    \n",
    "    y = Concatenate()([mf_vec, mlp_vector])\n",
    "    y = Dense(1, activation=\"sigmoid\", name=\"pred\")(y)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_input, profile_input, item_input,category_input], outputs=y)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.01),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "dims = 20\n",
    "model_neumffeat = get_model_neumffeat(NUM_USERS, NUM_ARTICLES, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fcd0bf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e358f14c1860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0musers_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticles_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcategories_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_categories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "users_input, articles_input, labels_input = np.array(train_users).reshape(-1,1), np.array(train_articles).reshape(-1,1), np.array(train_labels).reshape(-1,1)\n",
    "categories_input = np.array(train_categories).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#users_input, train_profiles, articles_input, categories_input\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "hits_list = []\n",
    "ndcg_list = []\n",
    "best_hits = 0\n",
    "best_ndcgs = 0\n",
    "best_hits_five = 0\n",
    "best_ndcgs_five = 0\n",
    "\n",
    "epochs=4\n",
    "for epoch in range(epochs):\n",
    "    hist = model_neumffeat.fit([users_input, train_profiles, articles_input, categories_input], labels_input, epochs=1, shuffle=True, verbose=1, batch_size=1024)\n",
    "    \n",
    "    train_loss.append(hist.history[\"loss\"])\n",
    "    train_acc.append(hist.history[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8728806",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = test.user_id.values\n",
    "test_items = test.article_id.values\n",
    "test_users, test_items = shuffle(test_users, test_items)\n",
    "test_users, test_items = test_users[:100], test_items[:100]\n",
    "test_set = zip(test_users, test_items)\n",
    "hits_ten,hits_five,ndcgs_ten,ndcgs_five = [], [], [], []\n",
    "for (u,i) in tqdm(test_set):\n",
    "    not_interacted_items = get_not_interacted(u)\n",
    "    users = np.array([u]*100)\n",
    "    items = np.array([i]+not_interacted_items)\n",
    "    profiles = np.tile(np.array(userid_to_profile[u]), 100).reshape(-1, 6)\n",
    "    categories = np.tile(np.array(article_to_category[i]), 100).reshape(-1,1)\n",
    "    \n",
    "    predictions = model_neumffeat.predict([users,profiles, items,categories])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [items[k] for k in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    hits_ten.append(getHitRatio(top_ten_items, i))\n",
    "    hits_five.append(getHitRatio(top_ten_items[:5], i))\n",
    "    ndcgs_ten.append(getNDCG(top_ten_items, i))\n",
    "    ndcgs_five.append(getNDCG(top_ten_items[:5], i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1600ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hit @ 10: {:.2f}\".format(np.average(hits_ten)))\n",
    "print(\"ncdgs @ 10: {:.2f}\".format(np.average(hits_five)))\n",
    "print(\"Hit @ 10: {:.2f}\".format(np.average(ndcgs_ten)))\n",
    "print(\"ncdgs @ 10: {:.2f}\".format(np.average(ndcgs_five)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c01ff1a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194116cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
