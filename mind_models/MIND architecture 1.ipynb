{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81340756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import scipy\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Input,Flatten, Embedding, Reshape, Multiply, Dropout, Dense, Concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Layer, SpatialDropout1D, GlobalMaxPooling1D, Bidirectional, GRU, LSTM\n",
    "from tensorflow.keras.layers import Dot, TimeDistributed, BatchNormalization, Add, Multiply\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#import keras.backend as K\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import math\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d44ae11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/mind_small/\"\n",
    "news = pd.read_csv(PATH + \"news.tsv\",header=None, sep=\"\\t\")\n",
    "behaviors = pd.read_csv(PATH + \"behaviors.tsv\", header=None, sep=\"\\t\")\n",
    "news.columns = [\"news_id\", \"category\", \"sub_category\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"]\n",
    "behaviors.columns = [\"idx\", \"user_id\", \"time\", \"history\", \"impressions\"]\n",
    "behaviors = behaviors.drop_duplicates([\"user_id\", \"history\"]) \n",
    "behaviors.dropna(subset=[\"user_id\", \"history\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "897d2bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = {}\n",
    "for idx, row in behaviors.iterrows():\n",
    "    sessions[row[\"user_id\"]] = row[\"history\"].split(\" \")\n",
    "\n",
    "users = []\n",
    "clicks = []\n",
    "for k, v in sessions.items():\n",
    "    for elem in v:\n",
    "        users.append(k)\n",
    "        clicks.append(elem)\n",
    "\n",
    "tuples = list(zip(users, clicks))\n",
    "interactions = pd.DataFrame(tuples, columns=[\"user\", \"news_id\"])\n",
    "interactions = interactions[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981bc38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>news_id</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U13740</td>\n",
       "      <td>N55189</td>\n",
       "      <td>tv</td>\n",
       "      <td>tvnews</td>\n",
       "      <td>'Wheel Of Fortune' Guest Delivers Hilarious, O...</td>\n",
       "      <td>We'd like to solve the puzzle, Pat: Blair Davi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAIORni.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U10045</td>\n",
       "      <td>N55189</td>\n",
       "      <td>tv</td>\n",
       "      <td>tvnews</td>\n",
       "      <td>'Wheel Of Fortune' Guest Delivers Hilarious, O...</td>\n",
       "      <td>We'd like to solve the puzzle, Pat: Blair Davi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAIORni.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U85394</td>\n",
       "      <td>N55189</td>\n",
       "      <td>tv</td>\n",
       "      <td>tvnews</td>\n",
       "      <td>'Wheel Of Fortune' Guest Delivers Hilarious, O...</td>\n",
       "      <td>We'd like to solve the puzzle, Pat: Blair Davi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAIORni.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U78244</td>\n",
       "      <td>N55189</td>\n",
       "      <td>tv</td>\n",
       "      <td>tvnews</td>\n",
       "      <td>'Wheel Of Fortune' Guest Delivers Hilarious, O...</td>\n",
       "      <td>We'd like to solve the puzzle, Pat: Blair Davi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAIORni.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U27024</td>\n",
       "      <td>N55189</td>\n",
       "      <td>tv</td>\n",
       "      <td>tvnews</td>\n",
       "      <td>'Wheel Of Fortune' Guest Delivers Hilarious, O...</td>\n",
       "      <td>We'd like to solve the puzzle, Pat: Blair Davi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAIORni.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user news_id category sub_category  \\\n",
       "0  U13740  N55189       tv       tvnews   \n",
       "1  U10045  N55189       tv       tvnews   \n",
       "2  U85394  N55189       tv       tvnews   \n",
       "3  U78244  N55189       tv       tvnews   \n",
       "4  U27024  N55189       tv       tvnews   \n",
       "\n",
       "                                               title  \\\n",
       "0  'Wheel Of Fortune' Guest Delivers Hilarious, O...   \n",
       "1  'Wheel Of Fortune' Guest Delivers Hilarious, O...   \n",
       "2  'Wheel Of Fortune' Guest Delivers Hilarious, O...   \n",
       "3  'Wheel Of Fortune' Guest Delivers Hilarious, O...   \n",
       "4  'Wheel Of Fortune' Guest Delivers Hilarious, O...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  We'd like to solve the puzzle, Pat: Blair Davi...   \n",
       "1  We'd like to solve the puzzle, Pat: Blair Davi...   \n",
       "2  We'd like to solve the puzzle, Pat: Blair Davi...   \n",
       "3  We'd like to solve the puzzle, Pat: Blair Davi...   \n",
       "4  We'd like to solve the puzzle, Pat: Blair Davi...   \n",
       "\n",
       "                                             url title_entities  \\\n",
       "0  https://assets.msn.com/labs/mind/AAIORni.html             []   \n",
       "1  https://assets.msn.com/labs/mind/AAIORni.html             []   \n",
       "2  https://assets.msn.com/labs/mind/AAIORni.html             []   \n",
       "3  https://assets.msn.com/labs/mind/AAIORni.html             []   \n",
       "4  https://assets.msn.com/labs/mind/AAIORni.html             []   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0  [{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...  \n",
       "1  [{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...  \n",
       "2  [{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...  \n",
       "3  [{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...  \n",
       "4  [{\"Label\": \"Pat Sajak\", \"Type\": \"P\", \"Wikidata...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = interactions.merge(news, on=[\"news_id\"])\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951b826d",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7af3f784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len before removal:  10000\n",
      "Len after removal:  9867\n"
     ]
    }
   ],
   "source": [
    "# remove users which have fewer than 5 interacations\n",
    "print(\"Len before removal: \",len(merged))\n",
    "_keys = merged[\"user\"].value_counts()[merged[\"user\"].value_counts() > 5].keys()\n",
    "merged = merged[merged[\"user\"].isin(_keys)]\n",
    "print(\"Len after removal: \",len(merged))\n",
    "\n",
    "\n",
    "user_enc = LabelEncoder()\n",
    "article_enc = LabelEncoder()\n",
    "merged[\"user_id\"] = user_enc.fit_transform(merged[\"user\"].values)\n",
    "merged[\"article_id\"] = article_enc.fit_transform(merged[\"news_id\"].values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f95b6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# Helper functions\n",
    "def _removeNonAscii(s):\n",
    "    return \"\".join(i for i in s if  ord(i)<128)\n",
    "\n",
    "def make_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def text_to_list(text):\n",
    "    text = text.split(\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b76a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title(df):\n",
    "    df[\"title_cleaned\"] = df.title.apply(func = make_lower_case)\n",
    "    df[\"title_cleaned\"] = df.title_cleaned.apply(func = remove_stop_words)\n",
    "    df[\"title_cleaned\"] = df.title_cleaned.apply(func = remove_punctuation)\n",
    "    return df\n",
    "def hyphen_to_underline(category):\n",
    "    \"\"\"\n",
    "    Convert hyphen to underline for the subcategories. So that Tfidf works correctly\n",
    "    \"\"\"\n",
    "    return category.replace(\"-\",\"_\")\n",
    "merged = clean_title(merged)\n",
    "merged[\"subcategory_cleaned\"] = merged[\"sub_category\"].apply(func = hyphen_to_underline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bc06877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9867x164 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9867 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=\"word\", tokenizer=str.split)\n",
    "item_ids = merged[\"article_id\"].unique().tolist()\n",
    "tfidf_matrix = vectorizer.fit_transform(merged[\"subcategory_cleaned\"])\n",
    "tfidf_feature_names = vectorizer.get_feature_names()\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8935feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ids = merged[\"article_id\"].tolist()\n",
    "\n",
    "def get_item_profile(item_id):\n",
    "    \"\"\"\n",
    "    item_id: the news article id\n",
    "    Return: an array of each n-gram in the item article. \n",
    "        with their n-gram id in tfidf_feature_names and weight.\n",
    "    \"\"\"\n",
    "    idx = item_ids.index(item_id) # returns the index to the item id\n",
    "    item_profile = tfidf_matrix[idx:idx+1]\n",
    "    return item_profile\n",
    "    \n",
    "def get_item_profiles(ids):\n",
    "    #print(ids)\n",
    "    item_profiles_list = [get_item_profile(x) for x in ids]\n",
    "    item_profiles = scipy.sparse.vstack(item_profiles_list)\n",
    "    return item_profiles\n",
    "\n",
    "def build_user_profile(person_id):\n",
    "    interactions = merged[merged[\"user_id\"] == person_id][\"article_id\"].values # gets all articles\n",
    "    user_item_profiles = get_item_profiles(interactions)\n",
    "    user_item_profiles = np.sum(user_item_profiles, axis=0)\n",
    "    user_profile_norm = sklearn.preprocessing.normalize(user_item_profiles)\n",
    "    return user_item_profiles\n",
    "    \n",
    "#t = build_user_profile(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5be56ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:08<00:00, 27.93it/s]\n"
     ]
    }
   ],
   "source": [
    "def calculate_user_profiles(unique_user_ids):\n",
    "    user_profiles = {}\n",
    "    for idx in tqdm(unique_user_ids):\n",
    "        token_relevance = build_user_profile(idx).tolist()[0]\n",
    "        zipped = zip(tfidf_feature_names, token_relevance)\n",
    "        s = sorted(zipped, key=lambda x: -x[-1])[:6]\n",
    "        user_profiles[idx] = s\n",
    "    return user_profiles\n",
    "        \n",
    "user_profiles = calculate_user_profiles(merged[\"user_id\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "363cc03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subcategory_to_id = {name: idx+1 for idx, name in enumerate(tfidf_feature_names)}\n",
    "id_to_subcategory = {idx: name for name, idx in subcategory_to_id.items()}\n",
    "id_to_subcategory[0] = \"Null\"\n",
    "subcategory_to_id[\"Null\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17cd6dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9867it [00:00, 11710.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# add all id-category to the userprofile in df\n",
    "profile_array = []\n",
    "for index, row in tqdm(merged.iterrows()):\n",
    "    \n",
    "    user_idx = row[\"user_id\"]\n",
    "    profile = user_profiles[user_idx]\n",
    "    temp = []\n",
    "    for keyword_tuple in profile:\n",
    "        temp.append(subcategory_to_id[keyword_tuple[0]])\n",
    "    profile_array.append(temp)\n",
    "merged[\"profile\"] = profile_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "521a6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the id-category to the news articles\n",
    "merged[\"subcategory_to_int\"] = [subcategory_to_id[cat] for cat in merged[\"subcategory_cleaned\"].values]\n",
    "\n",
    "user_unique = merged.drop_duplicates(\"user_id\")\n",
    "userid_to_profile = user_unique[[\"user_id\", \"profile\"]].set_index(\"user_id\").to_dict()[\"profile\"]\n",
    "\n",
    "category_enc = LabelEncoder()\n",
    "merged[\"main_category_int\"] = category_enc.fit_transform(merged[\"category\"].values)\n",
    "article_id_to_category_int = merged[[\"article_id\", \"main_category_int\"]].set_index(\"article_id\").to_dict()\n",
    "article_id_to_category_int = article_id_to_category_int[\"main_category_int\"]\n",
    "\n",
    "article_id_to_subcategory_int = merged[[\"article_id\", \"subcategory_to_int\"]].set_index(\"article_id\").to_dict()\n",
    "article_id_to_subcategory_int = article_id_to_subcategory_int[\"subcategory_to_int\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44738819",
   "metadata": {},
   "source": [
    "# 2. Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a2ae105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, user_id, article_id, have_timestamp, timestamp):\n",
    "    \"\"\"\n",
    "    params: \n",
    "        col_1: user_id\n",
    "        col_2: article_id\n",
    "    \"\"\"\n",
    "    df_test = df\n",
    "    if have_timestamp: # if df have timestamp; take last interacted article into test set\n",
    "        df_test = df_test.sort_values(timestamp).groupby(user_id).tail(1)\n",
    "    else:\n",
    "        df_test = df_test.sort_values(user_id).groupby(user_id).tail(1)\n",
    "    df_train = df.drop(index=df_test.index)\n",
    "    \n",
    "    assert df_test.shape[0] + df_train.shape[0] == df.shape[0]\n",
    "    \n",
    "    return df_train, df_test\n",
    "df_train_true, df_test_true = train_test_split(merged, \"user_id\", \"article_id\", False, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fa99490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9500/9500 [00:00<00:00, 49617.70it/s]\n"
     ]
    }
   ],
   "source": [
    "all_article_ids = merged[\"article_id\"].unique()\n",
    "\n",
    "def negative_sampling(train_df, all_article_ids, user_id, article_id):\n",
    "    \"\"\"\n",
    "    Negative sample training instance; for each positive instance, add 4 negative articles\n",
    "    \n",
    "    Return user_ids, news_ids, category_1, category_2, authors_onehotencoded, titles\n",
    "    \"\"\"\n",
    "    \n",
    "    users, articles, labels = [], [], []\n",
    "    user_item_set = set(zip(train_df[user_id], \n",
    "                            train_df[article_id]))\n",
    "    num_negatives = 4\n",
    "\n",
    "    for (u, i) in tqdm(user_item_set):\n",
    "        users.append(u)        \n",
    "        articles.append(i)\n",
    "        labels.append(1)\n",
    "        for _ in range(num_negatives):\n",
    "            negative_item = np.random.choice(all_article_ids)\n",
    "            while (u, negative_item) in user_item_set:\n",
    "                negative_item = np.random.choice(all_article_ids)\n",
    "            users.append(u)\n",
    "            articles.append(negative_item)\n",
    "            labels.append(0)\n",
    "    \n",
    "    users, articles, labels = shuffle(users, articles, labels, random_state=0)\n",
    "\n",
    "    return pd.DataFrame(list(zip(users, articles, labels)), columns=[\"user_id\",\"article_id\", \"labels\"])\n",
    "\n",
    "df_train = negative_sampling(df_train_true, all_article_ids, \"user_id\", \"article_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c15d2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:00<00:00, 986.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# For each user; for each item the user has interacted with in the test set;\n",
    "    # Sample 99 items the user has not interacted with in the past and add the one test item  \n",
    "    \n",
    "def negative_sample_testset(ordiginal_df, df_test, all_article_ids, user_id, article_id):\n",
    "    test_user_item_set = set(zip(df_test[user_id], df_test[article_id]))\n",
    "    user_interacted_items = ordiginal_df.groupby(user_id)[article_id].apply(list).to_dict()\n",
    "    users = []\n",
    "    res_arr = []\n",
    "    \n",
    "    userid_to_true_item = {} # keep track of the real items\n",
    "    for (u,i) in tqdm(test_user_item_set):\n",
    "        interacted_items = user_interacted_items[u]\n",
    "        not_interacted_items = set(all_article_ids) - set(interacted_items)\n",
    "        selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99))\n",
    "        test_items = selected_not_interacted + [i]\n",
    "        temp = []\n",
    "        for j in range(len(test_items)):\n",
    "            temp.append([u, test_items[j]])\n",
    "        \n",
    "        res_arr.append(temp)\n",
    "        userid_to_true_item[u] = i\n",
    "    X_test = np.array(res_arr)\n",
    "    X_test = X_test.reshape(-1, X_test.shape[-1])\n",
    "    df_test = pd.DataFrame(X_test, columns=[\"user_id\",\"article_id\"])\n",
    "    return X_test, df_test, userid_to_true_item\n",
    "X_test, df_test, userid_to_true_item = negative_sample_testset(merged, df_test_true, merged[\"article_id\"].unique(), \"user_id\", \"article_id\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08178dff",
   "metadata": {},
   "source": [
    "# 3. Evaluation seutp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5026635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "728c760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_rating(model, user_id, all_articles, true_item):\n",
    "    ### Reshaping to make it on the right shape ###\n",
    "    expanded_user_id = np.array([user_id]*100).reshape((100,1))\n",
    "    all_articles = np.array(all_articles).reshape(-1,1)\n",
    "    \n",
    "    # predictions\n",
    "    predictions = model.predict([expanded_user_id, all_articles])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [all_articles[i] for i in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    hr = getHitRatio(top_ten_items, true_item)\n",
    "    ndcg = getNDCG(top_ten_items, true_item)\n",
    "    return hr, ndcg\n",
    "\n",
    "def evalaute_model(model, df_test, userid_to_true_item):\n",
    "    print(\"Evaluate model\")\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    users = df_test[\"user_id\"].unique()\n",
    "    for user_id in tqdm(users):\n",
    "        user_df = df_test[df_test[\"user_id\"] == user_id] # get the 100 samples for this user\n",
    "        true_item = userid_to_true_item[user_id] # get the actual true item in the test set\n",
    "        all_articles = user_df[\"article_id\"].values # get all possible articles\n",
    "        \n",
    "        ht, ndcg = evaluate_one_rating(model, user_id, all_articles, true_item)\n",
    "        hits.append(ht)\n",
    "        ndcgs.append(ndcg)\n",
    "    return hits, ndcgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2adb73",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9154b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "num_users = len(merged[\"user_id\"].unique())\n",
    "num_items = len(merged[\"article_id\"].unique()) \n",
    "dims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc624f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.autograph.experimental.do_not_convert\n",
    "def get_model(num_users, num_items, dims, dense_layers=[128, 64, 32, 8]):\n",
    "    user_input = Input(shape=(1,), name=\"user\")\n",
    "    item_input = Input(shape=(1,), name=\"item\")\n",
    "    \n",
    "    user_emb = Embedding(output_dim=dims, input_dim=num_users, input_length=1, name=\"mf_user_emb\")(user_input)\n",
    "    item_emb = Embedding(output_dim=dims, input_dim=num_items, input_length=1, name=\"mf_item_emb\")(item_input)\n",
    "    \n",
    "    user_vecs = Reshape([dims])(user_emb)\n",
    "    item_vecs = Reshape([dims])(item_emb)\n",
    "    \n",
    "    y = Dot(1, normalize=False)([user_vecs, item_vecs])\n",
    "    \n",
    "    y = Dense(1, activation=\"sigmoid\")(y)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input], outputs=y)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.01),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = get_model(num_users, num_items, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d637b1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47500, 1) (47500, 1) (47500, 1)\n"
     ]
    }
   ],
   "source": [
    "###### Training ########\n",
    "user_input = df_train.iloc[:, 0].values.reshape((-1,1))\n",
    "item_input = df_train.iloc[:, 1].values.reshape((-1,1))\n",
    "labels = df_train.iloc[:, 2].values.reshape((-1,1))\n",
    "print(user_input.shape, item_input.shape, labels.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0491bb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336/1336 [==============================] - 2s 1ms/step - loss: 0.1717 - accuracy: 0.7990 - val_loss: 0.1563 - val_accuracy: 0.8004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/231 [00:00<00:25,  9.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:07<00:00, 32.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  44/1336 [..............................] - ETA: 1s - loss: 0.1033 - accuracy: 0.8388"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336/1336 [==============================] - 1s 956us/step - loss: 0.0911 - accuracy: 0.8748 - val_loss: 0.1905 - val_accuracy: 0.7413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/231 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:07<00:00, 30.89it/s]\n"
     ]
    }
   ],
   "source": [
    "all_user_ids = df_train[\"user_id\"].unique()\n",
    "\n",
    "#user_input = df_train.iloc[:, 0].values.reshape((-1,1))\n",
    "#profile_input = df_train.iloc[:, 1:6].values\n",
    "#item_input = df_train.iloc[:, 7].values.reshape((-1,1))\n",
    "#labels = df_train.iloc[:, 8].values.reshape((-1,1))\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "hits_list = []\n",
    "ndcg_list = []\n",
    "best_hits = 0\n",
    "best_ndcgs = 0\n",
    "\n",
    "epochs=2\n",
    "for epoch in range(epochs):\n",
    "    hist = model.fit([user_input, item_input], labels, epochs=1, shuffle=True, verbose=1, validation_split=0.1)\n",
    "    \n",
    "    train_loss.append(hist.history[\"loss\"])\n",
    "    train_acc.append(hist.history[\"accuracy\"])\n",
    "    val_loss.append(hist.history[\"val_loss\"])\n",
    "    val_acc.append(hist.history[\"val_accuracy\"])\n",
    "    \n",
    "    hits, ndcgs = evalaute_model( model, df_test, userid_to_true_item)\n",
    "    hits_list.append(np.average(hits))\n",
    "    ndcg_list.append(np.average(ndcgs))\n",
    "    \n",
    "    temp_hits = np.average(hits)\n",
    "    temp_ndcgs = np.average(ndcgs)\n",
    "    if (temp_hits > best_hits):\n",
    "        best_hits = temp_hits\n",
    "    if temp_ndcgs > best_ndcgs:\n",
    "        best_ndcgs = temp_ndcgs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e569e47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit @ 10: 0.24 \n",
      "Ndcg @ 10: 0.15\n"
     ]
    }
   ],
   "source": [
    "print(\"Hit @ 10: {:.2f} \\nNdcg @ 10: {:.2f}\".format(best_hits, best_ndcgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1049ebc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d690b3f4",
   "metadata": {},
   "source": [
    "# 5. user profile eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb2b67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(user_id, df):\n",
    "    \n",
    "    ## Setup ###\n",
    "    display_items = df[df[\"user_id\"] == user_id][\"article_id\"].values.reshape(-1, 1)\n",
    "    user_ids = np.tile(np.array(user_id), display_items.shape[0]).reshape(-1,1)\n",
    "    \n",
    "    ## Preds ###\n",
    "    predictions = model.predict([user_ids, display_items])\n",
    "    predicted_labels = np.squeeze(predictions)\n",
    "    top_ten_items = [display_items[i][0] for i in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    return top_ten_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bbe9c4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_subcategory(article_id, df):\n",
    "    \"\"\"\n",
    "    Return the article's category\n",
    "        type: int\n",
    "    \"\"\"\n",
    "    return article_id_to_subcategory_int[article_id]\n",
    "def get_category_hit_ratio(user_profile, top_ten_categories):\n",
    "    for profile in user_profile:\n",
    "        for category in top_ten_categories:\n",
    "            if profile == category:\n",
    "                return 1\n",
    "    return 0\n",
    "def get_ndcgs_category(user_profile, top_ten_categories):\n",
    "    for i in range(len(top_ten_categories)):\n",
    "        item = top_ten_categories[i]\n",
    "        for profile in user_profile:\n",
    "            if item == profile:\n",
    "                return math.log(2) / math.log(i+2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3b20b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:08<00:00, 27.45it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_category_hits_ndcg(df):\n",
    "    hits_ten,ndcgs_ten = [], []\n",
    "    hits_five, ndcgs_five = [], []\n",
    "    \n",
    "    for user_id in tqdm(df[\"user_id\"].unique()):\n",
    "        top_ten_articles = get_recommendations(user_id, df)\n",
    "        top_ten_subcategories = [get_article_subcategory(_id, df) for _id in top_ten_articles]\n",
    "        user_profile = userid_to_profile[user_id]\n",
    "\n",
    "        hit_ten = get_category_hit_ratio(user_profile, top_ten_subcategories)\n",
    "        ndcg_ten = get_ndcgs_category(user_profile, top_ten_subcategories)\n",
    "        \n",
    "        hit_five = get_category_hit_ratio(user_profile, top_ten_subcategories[:5])\n",
    "        ndcg_five = get_ndcgs_category(user_profile, top_ten_subcategories[:5])\n",
    "        \n",
    "        hits_ten.append(hit_ten)\n",
    "        ndcgs_ten.append(ndcg_ten)\n",
    "        \n",
    "        hits_five.append(hit_five)\n",
    "        ndcgs_five.append(ndcg_five)\n",
    "        \n",
    "        \n",
    "    return np.average(hits_ten), np.average(ndcgs_ten), np.average(hits_five), np.average(ndcg_five)\n",
    "        \n",
    "        \n",
    "category_hits_ten, category_ndcg_ten, category_hits_five, category_ndcg_five = get_category_hits_ndcg(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4401e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n",
      "0.5834249535370122\n",
      "0.7316017316017316\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(category_hits_ten)\n",
    "print(category_ndcg_ten)\n",
    "print(category_hits_five)\n",
    "print(category_ndcg_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cf4fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
